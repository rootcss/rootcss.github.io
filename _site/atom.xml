<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
    <title type="text" xml:lang="en">Shekhar Singh | All things Backend. Services, Big Data, Blockchain | Consultant, Freelancer</title>
    <link type="application/atom+xml" rel="self" href="http://localhost:3000/atom.xml"/>
  
  <link href="http://localhost:3000/"/>
  <id>http://localhost:3000/</id>
  <updated>2018-08-15T23:32:58Z</updated>
  <author>
    <name>Shekhar Singh</name>
    <email>shekhar.singh@msn.com</email>
  </author>
  <rights type="text">Copyright © 2018 Shekhar Singh. All rights reserved.</rights>
  
  <entry>
  <title type="text">Configuring SSL step by step (and Amazon Certificate Manager)</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2018/08/15/configuring-ssl-step-by-step-and-amazon-certificate-manager.html" />
  <id>http://localhost:3000/blog/2018/08/15/configuring-ssl-step-by-step-and-amazon-certificate-manager</id>
  <published>2018-08-15T00:00:00Z</published>
  <updated>2018-08-15T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>Configuring SSL for your domains is still not as simple as it can be. Everytime I do that, I need to refer to my previous notes. Recently, I was using <a href="https://aws.amazon.com/certificate-manager/" target="_blank"><code class="highlighter-rouge">AWS Certificate Manager</code></a> to setup a PositiveSSL Wildcard certificate, so I thought of putting up my notes on the blog.</p>

<p>Note: This post focuses on configuring SSL, and very less on details about what &amp; why.</p>

<p><b>Step 1: Generate Certificate Signing Request (CSR) and Private key</b></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>openssl req <span class="nt">-new</span> <span class="nt">-newkey</span> rsa:2048 <span class="nt">-nodes</span> <span class="nt">-keyout</span> mydomain.key <span class="nt">-out</span> mydomain.csr
</code></pre></div></div>

<p>Enter the details like country, state etc when asked. After this step you will have two files:</p>

<ul>
  <li>mydomain.csr (the CSR file)</li>
  <li>mydomain.key (the private key)</li>
</ul>

<p>Content of these files look like following:</p>

<p>mydomain.csr</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----BEGIN CERTIFICATE REQUEST-----
:
-----END CERTIFICATE REQUEST-----
</code></pre></div></div>

<p>mydomain.key</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----BEGIN PRIVATE KEY-----
:
-----END PRIVATE KEY-----
</code></pre></div></div>

<p><br />
<b>Step 2: Buying the certificate from providers</b></p>

<p>When requesting for your SSL certificate on the providers like namecheap, godaddy etc, you’ll be asked to enter your CSR content. Once you complete all the steps, you’ll receive following files from the provider (I’m taking the example of files from Namecheap where I purchased the  certificate)</p>

<ol>
  <li>AddTrustExternalCARoot.crt [Root CA Certificate]</li>
  <li>COMODORSAAddTrustCA.crt [Intermediate CA Certificate]</li>
  <li>COMODORSADomainValidationSecureServerCA.crt [Intermediate CA Certificate]</li>
  <li>STAR_mydomain.crt [Your PositiveSSL Certificate]</li>
</ol>

<p><br />
<b>Step 3: Creating SSL Bundle</b></p>

<p>Using the files mentioned in Step 2, we’ll be creating a SSL bundle, which is very simple. Just concatenate the content of first three files in right order as mentioned in the command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat </span>COMODORSADomainValidationSecureServerCA.crt COMODORSAAddTrustCA.crt AddTrustExternalCARoot.crt <span class="o">&gt;</span> ssl-bundle.crt
</code></pre></div></div>

<p>If you want to configure it for NGINX, then you need to concatenate your PositiveSSL certificate as well.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat </span>STAR_mydomain.crt COMODORSADomainValidationSecureServerCA.crt COMODORSAAddTrustCA.crt AddTrustExternalCARoot.crt <span class="o">&gt;</span> ssl-bundle.crt
</code></pre></div></div>

<p>Content of the bundle file will look something like this: (for ACM, only three entries will be there)</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----BEGIN CERTIFICATE-----
: - STAR_mydomain.crt
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
: - COMODORSADomainValidationSecureServerCA.crt
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
: - COMODORSAAddTrustCA.crt
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
: - AddTrustExternalCARoot.crt
-----END CERTIFICATE-----
</code></pre></div></div>

<p>Note:</p>
<ul>
  <li>The order of above files is very important.</li>
  <li>There shouldn’t be empty lines or line break between the certificates.</li>
</ul>

<p><br />
<b>Step 4: Configuring on NGINX</b></p>

<p>If you wanted to configure on NGINX, you just need these two files:</p>
<ul>
  <li>ssl-bundle.crt</li>
  <li>mydomain.key</li>
</ul>

<p>In you NGINX configuration, point these parameters to right path, restart NGINX and you’re good to go:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssl_certificate /etc/nginx/ssl/mydomain/ssl-bundle.crt;
ssl_certificate_key /etc/nginx/ssl/mydomain/mydomain.key;
</code></pre></div></div>

<p><br />
<b>Step 5: Configuring on Amazon Certificate Manager (ACM)</b></p>

<p>To configure it with ACM, you need to go through couple of more steps, as they required the certificates to be in certain format.
First convert your private key to .pem format:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>openssl rsa <span class="nt">-in</span> mydomain.key <span class="nt">-text</span> <span class="o">&gt;</span> mydomain-private-key.pem
</code></pre></div></div>

<p>Content:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----BEGIN RSA PRIVATE KEY-----
:
-----END RSA PRIVATE KEY-----
</code></pre></div></div>

<p>At last, to upload these certificates on ACM, use the following command:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws acm import-certificate <span class="se">\</span>
	<span class="nt">--certificate</span> file:///Users/rootcss/Downloads/ssl/STAR_mydomain.crt <span class="se">\</span>
	<span class="nt">--private-key</span> file:///Users/rootcss/Downloads/ssl/mydomain-private-key.pem <span class="se">\</span>
	<span class="nt">--certificate-chain</span> file:///Users/rootcss/Downloads/ssl/ssl-bundle.crt
</code></pre></div></div>

<p><br />
<b>Other notes:</b></p>

<ol>
  <li>If your private key is of format .pfx, you can convert it to .pem format directly using the command:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>openssl pkcs12 <span class="nt">-in</span> mydomain-private-key.pfx <span class="nt">-out</span> mydomain-private-key.pem <span class="nt">-nodes</span>
</code></pre></div>    </div>
    <p>(Enter the created password when asked)</p>
  </li>
  <li>If you need to convert your CSR file into PEM format, you can use the command:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>openssl x509 <span class="nt">-inform</span> PEM <span class="nt">-in</span> ssl-bundle.crt <span class="o">&gt;</span> ssl-bundle.crt
</code></pre></div>    </div>
  </li>
</ol>

<p><br />
<b>References:</b></p>
<ul>
  <li>https://aws.amazon.com/certificate-manager/</li>
  <li>http://nginx.org/en/docs/http/configuring_https_servers.html</li>
  <li>https://stackoverflow.com/questions/991758/how-to-get-pem-file-from-key-and-crt-files</li>
</ul>
 ]]></content>
</entry>


  <entry>
  <title type="text">Observations on querying Cassandra on 'multiple' partitions (with/without Spark)</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2017/04/01/querying-cassandra-from-spark-on-multiple-paritions.html" />
  <id>http://localhost:3000/blog/2017/04/01/querying-cassandra-from-spark-on-multiple-paritions</id>
  <published>2017-04-01T00:00:00Z</published>
  <updated>2017-04-01T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>Cassandra’s brilliancy totally depends on your data models. You should know beforehand about how the data will be accessed/queried; and then design accordingly.</p>

<p>If you’re querying a Cassandra table, you are going to start writing your query with the <b>partition key</b>, because as we know, the partition key tells about the data locality in the cluster. Writing a query that includes multiple partition keys is never optimized, because those keys might be on different nodes. Just assume, you have 500 nodes with RF=3 and each node is being scanned for those partition keys.</p>

<p>It’s going to be super expensive.</p>

<p>For example, Let’s say, I have a <code class="highlighter-rouge">users</code> table like this:</p>

<p>(partition key - id, clustering key - event_timestamp)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>id | event_timestamp | city
1  | abc............ | A
1  | def............ | B
2  | abc............ | B
3  | abc............ | C
:
:
</code></pre></div></div>

<p>Now, If I write a query like:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>This is perfectly optimized, and thanks to our Murmur3 partitioner we will get the result instantly.</p>

<p><br />
However, if I write a query like:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="k">IN</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">123</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">457</span><span class="p">,</span> <span class="mi">58</span><span class="p">,</span> <span class="mi">768</span><span class="p">,</span> <span class="mi">5435</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">547</span><span class="p">,</span> <span class="mi">456</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">2342</span><span class="p">,</span> <span class="mi">34</span><span class="p">....)</span>
</code></pre></div></div>

<p>On a small cluster this will cause no major issues, but on a 500 nodes cluster, it’s going to affect the JVM’s Heap badly, as explained above.</p>

<p><br />
Now, coming to <b>Spark</b>.</p>

<p>On a small scale, you wouldn’t even notice the problem. Not just with Spark, but even with CQLSH you wouldn’t notice the delay and issues significantly.
However, If your cluster is significantly large, it will be very slow and highly unoptimized, and we don’t really like that, right.</p>

<p><a href="https://github.com/datastax/spark-cassandra-connector" target="_blank"><code class="highlighter-rouge">cassandra-spark-connector</code></a> has a method called <code class="highlighter-rouge">joinWithCassandraTable()</code> to which you can pass a list of partition keys to be looked up.</p>

<p>Internally, this method extracts all the partition keys from the list, and runs a separate parallel query (spark tasks) for each partition key on our “distributed” Spark cluster (it uses Cassandra Java driver to perform this operation). Finally returns an RDD object consisting of results from all tasks.</p>

<p>So, our 2nd query was converted into something like this,</p>
<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">4</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">123</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">25</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">345</span>
<span class="p">:</span>
</code></pre></div></div>

<p>Usage of the method:</p>
<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="n">myList</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">partition_keys</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="nc">Tuple1</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>
<span class="k">val</span> <span class="n">myResult</span> <span class="k">=</span> <span class="n">myList</span><span class="o">.</span><span class="n">joinWithCassandraTable</span><span class="o">(</span><span class="n">keyspace</span><span class="o">,</span> <span class="s">"users"</span><span class="o">)</span>
</code></pre></div></div>

<p>We cannot say this is an extremely optimized solution, but considering the huge number of advantages that we get from Cassandra, we can compromise a bit here ;-)</p>

<p>And by the way, this method is not yet available for Pyspark, only in Scala. I am attempting to write one for Pyspark, will be sharing the details soon.</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">Spark Shell for Processing &amp;amp; Querying data in Cassandra</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2017/01/24/processing-cassandra-data-with-apache-spark-part-2.html" />
  <id>http://localhost:3000/blog/2017/01/24/processing-cassandra-data-with-apache-spark-part-2</id>
  <published>2017-01-24T00:00:00Z</published>
  <updated>2017-01-24T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>My previous <a href="processing-cassandra-data-with-apache-spark.html">post</a>  explains, how can you write a Spark job and execute it. In this post, I am writing down steps to initiate a Spark shell (pyspark or spark-shell), with a pre-established connection to Cassandra. In addition to this, I’ll write down some sample codes and their outputs, in order to show the usage of Spark Transformations/Actions.</p>

<p>To start the shell, just run this command on your shell.
<br /></p>
<pre>
pyspark \
      --packages com.datastax.spark:spark-cassandra-connector_2.10:1.5.0-M2 \
      --conf spark.cassandra.connection.host=192.168.56.101 \
      --conf spark.cassandra.auth.username=cassandra \
      --conf spark.cassandra.auth.password=cassandra
</pre>

<p>I think, most of the parameters are pretty intuitive. In short, we are just providing the dependency packages and cassandra connection configurations. Make sure you provide path to <code class="highlighter-rouge">pyspark</code> or add it in your <code class="highlighter-rouge">$PATH</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Python 2.7.11 <span class="o">(</span>default, Nov 10 2016, 03:37:47<span class="o">)</span>
<span class="o">[</span>GCC 4.2.1 Compatible Apple LLVM 8.0.0 <span class="o">(</span>clang-800.0.42.1<span class="o">)]</span> on darwin

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _<span class="se">\ \/</span> _ <span class="se">\/</span> _ <span class="sb">`</span>/ __/  <span class="s1">'_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Python version 2.7.11 (default, Nov 10 2016 03:37:47)
SparkContext available as sc, HiveContext available as sqlContext.


&gt;&gt;&gt; sqlContext.sql("""CREATE TEMPORARY TABLE roles \
...                   USING org.apache.spark.sql.cassandra \
...                   OPTIONS ( table "roles", \
...                             keyspace "system_auth", \
...                             cluster "rootCSSCluster", \
...                             pushdown "true") \
...               """)
DataFrame[]


&gt;&gt;&gt; sqlContext.sql('</span>SELECT <span class="k">*</span> from roles<span class="s1">').show()
+---------+---------+------------+---------+--------------------+
|     role|can_login|is_superuser|member_of|         salted_hash|
+---------+---------+------------+---------+--------------------+
|cassandra|     true|        true|       []|$2a$10$pQW3iGSC.m...|
+---------+---------+------------+---------+--------------------+


&gt;&gt;&gt; sqlContext.sql("""CREATE TEMPORARY TABLE compaction_history \
...                   USING org.apache.spark.sql.cassandra \
...                   OPTIONS ( table "compaction_history", \
...                             keyspace "system", \
...                             cluster "rootCSSCluster", \
...                             pushdown "true") \
...               """)
DataFrame[]


&gt;&gt;&gt; sqlContext.sql('</span>SELECT <span class="k">*</span> FROM compaction_history LIMIT 3<span class="s1">').show()
+--------------------+--------+---------+-----------------+--------------------+-------------+
|                  id|bytes_in|bytes_out|columnfamily_name|        compacted_at|keyspace_name|
+--------------------+--------+---------+-----------------+--------------------+-------------+
|b54ccf00-e236-11e...|   20729|     4301|   size_estimates|2017-01-24 18:42:...|       system|
|170ddba0-e23f-11e...|   12481|     3085| sstable_activity|2017-01-24 19:42:...|       system|
|914d2e70-e195-11e...|      41|        0|  schema_triggers|2017-01-23 23:28:...|       system|
+--------------------+--------+---------+-----------------+--------------------+-------------+


&gt;&gt;&gt; sqlContext.sql('</span>SELECT columnfamily_name, COUNT<span class="o">(</span><span class="k">*</span><span class="o">)</span> AS count FROM compaction_history GROUP BY columnfamily_name<span class="s1">').show()
+-----------------+-----+
|columnfamily_name|count|
+-----------------+-----+
| schema_functions|    2|
|schema_aggregates|    2|
|  schema_triggers|    2|
| schema_usertypes|    2|
|   size_estimates|    2|
| sstable_activity|    4|
+-----------------+-----+


&gt;&gt;&gt; sqlContext.sql('</span>SELECT SUM<span class="o">(</span>bytes_in<span class="o">)</span> AS <span class="nb">sum </span>FROM compaction_history<span class="s1">').show()
+-----+
|  sum|
+-----+
|73610|
+-----+


&gt;&gt;&gt; df_payload = sqlContext.sql('</span>SELECT bytes_in, bytes_out, columnfamily_name FROM compaction_history<span class="s1">')
&gt;&gt;&gt; df_payload.count()
14


# Simple Map Reduce example
&gt;&gt;&gt; df_payload\
...      .map(lambda x: (x['</span>columnfamily_name<span class="s1">'], 1))\
...      .reduceByKey(lambda x, y: x+y)\
...      .toDF()\
...      .show()
+-----------------+---+
|               _1| _2|
+-----------------+---+
|   size_estimates|  2|
| sstable_activity|  4|
| schema_usertypes|  2|
|schema_aggregates|  2|
| schema_functions|  2|
|  schema_triggers|  2|
+-----------------+---+

&gt;&gt;&gt;

</span></code></pre></div></div>

<p><br />
I’ll be adding more examples with time. :)</p>

<style>
pre code{
  white-space: pre;
}
</style>

 ]]></content>
</entry>


  <entry>
  <title type="text">Processing &amp;amp; Querying data in Cassandra with Apache Spark</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2017/01/23/processing-cassandra-data-with-apache-spark.html" />
  <id>http://localhost:3000/blog/2017/01/23/processing-cassandra-data-with-apache-spark</id>
  <published>2017-01-23T00:00:00Z</published>
  <updated>2017-01-23T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>This post is one of my <a href="new.html">Notes to Self</a> one. I’m simply going to write, how can you connect to Cassandra from Spark, run “SQL” queries and perform analysis on Cassandra’s data.</p>

<p><br />
Let’s get started.</p>

<p><br />
(Platform: Spark v1.6.0, Cassandra v2.7, macOS 10.12.1, Scala 2.11.7)</p>

<p>I’m going to use the package <code class="highlighter-rouge">spark-cassandra-connector</code> written by awesome <a href="http://www.datastax.com/">Datastax</a> guys.</p>

<p>Assuming you have already configured Cassandra &amp; Spark, it’s time to start writing a small Spark job.</p>

<p><b>Code with explanation</b>:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># imports necessary methods</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">SparkConf</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SQLContext</span>


<span class="c"># setup spark configuration object</span>
<span class="c"># this contains your cassandra connection parameters</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span>\
    <span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="s">"PySpark Cassandra"</span><span class="p">)</span> \
    <span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.cassandra.connection.host"</span><span class="p">,</span> <span class="s">"192.168.56.101"</span><span class="p">)</span>\
    <span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.cassandra.auth.username"</span><span class="p">,</span> <span class="s">"cassandra"</span><span class="p">)</span>\
    <span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.cassandra.auth.password"</span><span class="p">,</span> <span class="s">"cassandra"</span><span class="p">)</span>


<span class="c"># creates Spark Context with your cassandra configurations</span>
<span class="c"># local[*] represents that spark is going to use all cores of CPU for this job</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="s">"local[*]"</span><span class="p">,</span> <span class="s">"PySpark Cassandra"</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>


<span class="c"># creates Spark's sqlContext. This is going to be super useful.</span>
<span class="n">sqlContext</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>


<span class="c"># creates mapping with tables inside Cassandra for Spark</span>
<span class="c"># I am going to use "system_auth.roles" table here</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"""CREATE TEMPORARY TABLE roles </span><span class="se">\
</span><span class="s">                  USING org.apache.spark.sql.cassandra </span><span class="se">\
</span><span class="s">                  OPTIONS ( table "roles", </span><span class="se">\
</span><span class="s">                            keyspace "system_auth", </span><span class="se">\
</span><span class="s">                            cluster "rootCSSCluster", </span><span class="se">\
</span><span class="s">                            pushdown "true") </span><span class="se">\
</span><span class="s">              """</span><span class="p">)</span>

<span class="c"># you can create multiple mappings/temporary tables and write queries on it.</span>
<span class="c"># this in another table: "system.compaction_history"</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"""CREATE TEMPORARY TABLE compaction_history </span><span class="se">\
</span><span class="s">                  USING org.apache.spark.sql.cassandra </span><span class="se">\
</span><span class="s">                  OPTIONS ( table "compaction_history", </span><span class="se">\
</span><span class="s">                            keyspace "system", </span><span class="se">\
</span><span class="s">                            cluster "rootCSSCluster", </span><span class="se">\
</span><span class="s">                            pushdown "true") </span><span class="se">\
</span><span class="s">              """</span><span class="p">)</span>


<span class="c"># here's the query we are going to run</span>
<span class="n">query</span> <span class="o">=</span> <span class="s">"SELECT * FROM roles"</span>

<span class="k">print</span> <span class="s">"[Spark] Executing query: </span><span class="si">%</span><span class="s">s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="c"># The result of the query returns a dataframe</span>
<span class="n">df_payload</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="c"># priting the content of dataframe</span>
<span class="n">df_payload</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p><br /></p>

<p><b>Spark job Execution</b>:</p>

<p>To run your spark job, use the command below:</p>
<pre>
spark-submit --packages com.datastax.spark:spark-cassandra-connector_2.10:1.5.0-M2 myfile.py
</pre>

<p>(Note: Check <code class="highlighter-rouge">localhost:4040</code> in your browser for Spark UI)</p>

<p>--packages : This parameter tells Spark to download the external dependencies for the job.</p>

<p>In our case, we are using <code class="highlighter-rouge">spark-cassandra-connector</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>groupId: com.datastax.spark
artifactId: spark-cassandra-connector_2.10
version: 1.5.0-M2
</code></pre></div></div>

<p><br /></p>

<p><b>Output</b>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;all your logs will be printed here. including ivy logs&gt;
:
:
<span class="o">[</span>Spark] Executing query: <span class="k">select</span> <span class="k">*</span> from roles
+---------+---------+------------+---------+--------------------+
|     role|can_login|is_superuser|member_of|         salted_hash|
+---------+---------+------------+---------+--------------------+
|cassandra|     <span class="nb">true</span>|        <span class="nb">true</span>|       <span class="o">[]</span>|<span class="nv">$2a$10$pQW3iGSC</span>.m...|
+---------+---------+------------+---------+--------------------+
</code></pre></div></div>

<p><br />
Here, <code class="highlighter-rouge">df_payload</code> is DataFrame object. You can use all Spark’s <i>Transformations</i> &amp; <i>Actions</i> on this. (Check <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">here</a> for more details)</p>

<p><br />
<b>Second Part</b>: <u>Starting a Spark shell with Cassandra connection</u>
<br />
Steps for this is part of separate <a href="processing-cassandra-data-with-apache-spark-part-2.html">post</a>.</p>

<hr />

<p><b>Useful Links </b>:-</p>

<ol>
  <li>
    <p>I really want to thank guys at <a href="http://www.datastax.com/">Datastax</a>. They have written and open sourced, so many packages and drivers for Cassandra.</p>
  </li>
  <li>
    <p>You can contribute to <code class="highlighter-rouge">spark-cassandra-connector</code> <a href="https://github.com/datastax/spark-cassandra-connector">here</a>.</p>
  </li>
  <li>
    <p><a href="https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10/1.5.0-M2">Link</a> to <code class="highlighter-rouge">spark-cassandra-connector</code> maven repository.</p>
  </li>
</ol>
 ]]></content>
</entry>


  <entry>
  <title type="text">Processing Rabbitmq's Stream with &quot;Apache Flink&quot;</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2016/11/12/apache-flink-rabbimq-streams-processor.html" />
  <id>http://localhost:3000/blog/2016/11/12/apache-flink-rabbimq-streams-processor</id>
  <published>2016-11-12T00:00:00Z</published>
  <updated>2016-11-12T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>I love <a target="_blank" href="http://spark.apache.org/">Apache Spark</a>. Not just becacuse of it’s capability to adapt to so many use-cases, but because it’s one of shining star in the <i>Distributing Computing</i> world, has a great design and superb community backing.</p>

<p>However, one of the features I’d want enhancement in is, the way it processes the streams. Spark processes the streams in a <b>micro-batch</b> manner i.e, you set a time interval (could be any value), and Spark will process the events collected in that interval, in batch. This is where Apache Flink comes in!</p>

<p><a target="_blank" href="http://spark.apache.org/">Apache Flink</a> is often comapred with Spark. I feel Spark is far ahead of Flink, not just in technology; but even community backing of Spark is very big, compared to Flink.</p>

<p>Anyways, this post is not about comparing them, but to provide a detailed example of processing a RabbitMQ’s stream using Apache Flink.</p>

<p><b>Step 1:</b> Install Rabbitmq, Apache Flink in your system. Both installations are very straightforward.</p>

<p><b>Step 2:</b> Start Rabbitmq server</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rabbitmq-server &amp;
</code></pre></div></div>

<p><b>Step 3:</b> Create an exchange in the rabbitmq. Go to <code class="highlighter-rouge">http://localhost:15672</code> (In my example, I’m binding a queue to the exchange. You can directly use a queue, but make sure to make corresponding changes in the code)</p>

<p><b>Step 4:</b> Clone the repo from <a target="_blank" href="https://github.com/rootcss/flink-rabbitmq.git ">here</a>: (will be explaining the codes inline)</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/rootcss/flink-rabbitmq.git
</code></pre></div></div>

<p><b>Step 5:</b> It’s built with maven. (Java) So, build it using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mvn clean package
</code></pre></div></div>

<p><b>Step 6:</b> Once built, You’re all set to run it now:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>flink run <span class="nt">-c</span> com.rootcss.flink.RabbitmqStreamProcessor target/flink-rabbitmq-0.1.jar
</code></pre></div></div>

<p><b>Step 7:</b> Check the logs at:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">tail</span> <span class="nt">-f</span> <span class="nv">$FLINK_HOME</span>/log/<span class="k">*</span>
</code></pre></div></div>

<p>and Flink’s dashboard at:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>http://localhost:8081/
</code></pre></div></div>

<p><b>Step 8:</b> Now, you can start publishing events from the RabbitMQ’s exchange and see the output in the logs.</p>

<p>Note that, I am not using any <b>Flink’s Sink</b> here (writing into the logs). You can use a file system like HDFS or a Database or even Rabbitmq (on a different channel ;))</p>

<h3 id="code-explanation">Code Explanation</h3>
<p>(This version might be a little different from the code in my repo. Just to keep this concise)</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Extend the RMQSource class, since we need to override a method to bind our queue</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">RabbitmqStreamProcessor</span> <span class="kd">extends</span> <span class="n">RMQSource</span><span class="o">{</span>

    <span class="c1">// This is mainly because we have to bind our queue to an exchange. If you are using a queue directly, you may skip it</span>
    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">setupQueue</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
        <span class="n">AMQP</span><span class="o">.</span><span class="na">Queue</span><span class="o">.</span><span class="na">DeclareOk</span> <span class="n">result</span> <span class="o">=</span> <span class="n">channel</span><span class="o">.</span><span class="na">queueDeclare</span><span class="o">(</span><span class="s">"simple_dev"</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>
        <span class="n">channel</span><span class="o">.</span><span class="na">queueBind</span><span class="o">(</span><span class="n">result</span><span class="o">.</span><span class="na">getQueue</span><span class="o">(),</span> <span class="s">"simple_exchange"</span><span class="o">,</span> <span class="s">"*"</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
        <span class="c1">// Setting up rabbitmq's configurations; ignore the default values</span>
        <span class="n">RMQConnectionConfig</span> <span class="n">connectionConfig</span> <span class="o">=</span> <span class="k">new</span> <span class="n">RMQConnectionConfig</span><span class="o">.</span><span class="na">Builder</span><span class="o">()</span>
                <span class="o">.</span><span class="na">setHost</span><span class="o">(</span><span class="s">"localhost"</span><span class="o">).</span><span class="na">setPort</span><span class="o">(</span><span class="mi">5672</span><span class="o">).</span><span class="na">setUserName</span><span class="o">(</span><span class="s">"rootcss"</span><span class="o">)</span>
                <span class="o">.</span><span class="na">setPassword</span><span class="o">(</span><span class="s">"password"</span><span class="o">).</span><span class="na">setVirtualHost</span><span class="o">(</span><span class="s">"/"</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>

        <span class="c1">// below ones are pretty intuitive class names, right?</span>
        <span class="n">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

        <span class="c1">// Finally adding Rabbitmq as source of the stream for Flink</span>
        <span class="n">DataStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">dataStream</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="k">new</span> <span class="n">RMQSource</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;(</span><span class="n">connectionConfig</span><span class="o">,</span>
                <span class="s">"simple_dev"</span><span class="o">,</span>
                <span class="k">new</span> <span class="nf">SimpleStringSchema</span><span class="o">()));</span>

        <span class="c1">// Accepting the events, and doing a flatMap to calculate string length of each event (to keep the things easy)</span>
        <span class="n">DataStream</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">dataStream</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">TextLengthCalculator</span><span class="o">());</span>

        <span class="c1">// action on the pairs, you can plug your Flink's Sink here as well.</span>
        <span class="n">pairs</span><span class="o">.</span><span class="na">print</span><span class="o">();</span>

        <span class="c1">// Start the execution of the worker</span>
        <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">();</span>
    <span class="o">}</span>
</code></pre></div></div>

<style>
pre code{
  white-space: pre;
}
</style>

<p>And, here is the beautiful web interface of Apache Flink:</p>

<p><img class="img-responsive" src="assets/images/2016-11-12-apache-flink-rabbimq-streams-processor_1.png" alt="Flink Web Dashboard" /></p>
<p><br />
In the next post, I will be explaining how I bomarded events on both Spark &amp; Flink, to compare their endurance. Just for fun :-D</p>

<p>Stay Tuned!</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">Writing Apache Spark workers with &quot;Simple Spark Lib&quot;</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2016/11/11/apache-spark-worker-with-simple-spark-lib.html" />
  <id>http://localhost:3000/blog/2016/11/11/apache-spark-worker-with-simple-spark-lib</id>
  <published>2016-11-11T00:00:00Z</published>
  <updated>2016-11-11T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p><a target="_blank" href="http://spark.apache.org/">Apache Spark</a> is a great project, could be plugged with most of the data sources/databases eg, HDFS, Cassandra, MongoDB, Kafka, Postgres, Redshift etc. I have been using Spark for ad-hoc querying, bunch of Aggregations &amp; Segregations over Cassandra from a long time and noticed that, every time I used to write (or paste) same code for configuration &amp; connection. Also, I knew when someone else wants to do the similar work from my team, he/she will have to do the same thing, including learning what that means and understanding it. Think of someone doing that, if he is using Spark for the first time?</p>

<p><br />TLDR;</p>

<p>I decided to write a wrapper over <code class="highlighter-rouge">PySpark</code> which obviously supports Cassandra, Redshift etc. It primarily provided following two advantages:</p>

<ol>
  <li>I never repeated myself while writing the workers again</li>
  <li>My Team members do not need to figure out those Spark specific code in order to do some simple ad-hoc tasks</li>
</ol>

<p>I named it “<code class="highlighter-rouge">Simple Spark Lib</code>” and, here’s how to use it:</p>

<p>Step 1: Clone the repo from <a href="https://github.com/rootcss/simple_spark_lib">here</a>:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">git clone https://github.com/rootcss/simple_spark_lib.git</code></pre></figure>

<p>Step 2: Install the library:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">python setup.py <span class="nb">install</span></code></pre></figure>

<p>Step 3: Write the worker:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># First, import the library</span>
<span class="kn">from</span> <span class="nn">simple_spark_lib</span> <span class="kn">import</span> <span class="n">SimpleSparkCassandraWorkflow</span>

<span class="c"># Define connection configuration for cassandra</span>
<span class="n">cassandra_connection_config</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'host'</span><span class="p">:</span>     <span class="s">'192.168.56.101'</span><span class="p">,</span>
  <span class="s">'username'</span><span class="p">:</span> <span class="s">'cassandra'</span><span class="p">,</span>
  <span class="s">'password'</span><span class="p">:</span> <span class="s">'cassandra'</span>
<span class="p">}</span>

<span class="c"># Define Cassandra Schema information</span>
<span class="n">cassandra_config</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'cluster'</span><span class="p">:</span> <span class="s">'rootCSSCluster'</span><span class="p">,</span>
  <span class="s">'tables'</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">'api_events'</span><span class="p">:</span> <span class="s">'events_production.api_events'</span><span class="p">,</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="c"># Initiate your workflow</span>
<span class="n">workflow</span> <span class="o">=</span> <span class="n">SimpleSparkCassandraWorkflow</span><span class="p">(</span><span class="n">appName</span><span class="o">=</span><span class="s">"Simple Example Worker"</span><span class="p">)</span>

<span class="c"># Setup the workflow with configurations</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">cassandra_connection_config</span><span class="p">,</span> <span class="n">cassandra_config</span><span class="p">)</span>

<span class="c"># Run your favourite query</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">workflow</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="s">"SELECT * FROM api_events LIMIT 10"</span><span class="p">)</span>

<span class="k">print</span> <span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p>Step 4: Save it &amp; Execute the worker:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">simple-runner my_spark_woker.py <span class="nt">-d</span> cassandra</code></pre></figure>

<p><code class="highlighter-rouge">simple_spark_lib</code> enables you to use the capability of spark without writing the actual Spark codes. I made it public, hoping it might be useful to someone else too.</p>

<p>If you are interested, go through other examples in the repo and feel free to contribute. :-)</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">RabbitMQ - Automated deletion of 1000s of queues</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2016/03/24/rabbitmq-queues-overflow-automated-deletion.html" />
  <id>http://localhost:3000/blog/2016/03/24/rabbitmq-queues-overflow-automated-deletion</id>
  <published>2016-03-24T00:00:00Z</published>
  <updated>2016-03-24T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>Recently, I was using <a href="https://github.com/jondot/sneakers">sneakers</a> for rails, which is a small framework for Ruby and RabbitMQ. One issue with sneakers is that, if you have faulty configuration for a queue or you do not provide a queue name, it leaves it upto rabbitmq to define it. So, for some reason (which I don’t want to focus on), we had more than 1600 queues created on that particular exchange, and unfortunately they were not Auto-delete and we didn’t want other exchanges and queues to get hurt because of this ;)</p>

<p>Anyway, Now the challenge was to delete them, because for that you have to manually click on each queue, select ‘Delete’ on the new page and finally, confirm it on pop-up.</p>

<p>I was like, :O</p>

<p>Anyways, Thanks to handy APIs of rabbitmq, these are the following few commands I used in order to delete them quickly. (rabbitmq generally creates default queues with names like <code class="highlighter-rouge">amq.gen--*</code>)</p>

<p>First let’s list all the queues in the form of bash arrays:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">rabbitmqadmin <span class="nt">--host</span><span class="o">=</span>&lt;mqserver.hostname.com&gt; <span class="nt">--port</span><span class="o">=</span>443 <span class="nt">--ssl</span> <span class="nt">--vhost</span><span class="o">=</span>&lt;your_vhost&gt; <span class="nt">--username</span><span class="o">=</span>&lt;your_username&gt; <span class="nt">--password</span><span class="o">=</span>&lt;your_password&gt; list queues | <span class="nb">awk</span> <span class="s1">'{print $2}'</span> | <span class="nb">grep </span>amq.gen  | xargs | <span class="nb">sed</span> <span class="nt">-e</span> <span class="s1">'s/ /" "/g'</span></code></pre></figure>

<p>Now copy the output of it, declare as an array and run a loop to delete them all.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">declare</span> <span class="nt">-a</span> <span class="nv">arr</span><span class="o">=(</span><span class="s2">"amq.gen--PxKpFBHkIxxebJEwbmV6g"</span> <span class="s2">"amq.gen--Q6BeLdfGHsXY6RgVmu8Ig"</span> <span class="s2">"amq.gen--WI0hRAHCOkPIrEULYc1vQ"</span> <span class="s2">"amq.gen--XufS0RrnfZUXyf0Rt1tAg"</span> <span class="s2">"amq.gen--_NXdwlSHYDJwGDiuX8_XA"</span> ......<span class="o">)</span>

<span class="k">for </span>i <span class="k">in</span> <span class="s2">"</span><span class="k">${</span><span class="nv">arr</span><span class="p">[@]</span><span class="k">}</span><span class="s2">"</span>
<span class="k">do
   </span><span class="nb">echo</span> <span class="s2">"</span><span class="nv">$i</span><span class="s2">"</span>
   rabbitmqadmin <span class="nt">--host</span><span class="o">=</span>&lt;mqserver.hostname.com&gt; <span class="nt">--port</span><span class="o">=</span>443 <span class="nt">--ssl</span> <span class="nt">--vhost</span><span class="o">=</span>&lt;your_vhost&gt; <span class="nt">--username</span><span class="o">=</span>&lt;your_username&gt; <span class="nt">--password</span><span class="o">=</span>&lt;your_password&gt; delete queue <span class="nv">name</span><span class="o">=</span><span class="s2">"</span><span class="nv">$i</span><span class="s2">"</span>
<span class="k">done</span></code></pre></figure>

<p>That’s it. These small hacks makes me fall in love with programming everyday &lt;3</p>

<p>Thanks!</p>

<style type="text/css">
pre {
    white-space: pre-wrap;
}
</style>

 ]]></content>
</entry>


  <entry>
  <title type="text">Linux I/O redirection examples</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2016/02/15/linux-io-redirections.html" />
  <id>http://localhost:3000/blog/2016/02/15/linux-io-redirections</id>
  <published>2016-02-15T00:00:00Z</published>
  <updated>2016-02-15T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>I/O redirections are one of the prettiest things we have in linux (IMO!) Following are commands and their usage.
<br /><br />
<code class="highlighter-rouge">command_output &gt;&gt; file </code>
  Redirects stdout to a file. Creates the file if not present, otherwise appends.</p>

<p><code class="highlighter-rouge"> &gt; filename </code>
  Truncates the file to zero length. If file is not present, creates zero-length file (same effect as <code class="highlighter-rouge">touch</code>).</p>

<p><code class="highlighter-rouge"> 1&gt;filename </code>
  Redirects stdout to the file “filename”.</p>

<p><code class="highlighter-rouge"> 1&gt;&gt;filename </code>
  Redirects and appends stdout to file “filename”.</p>

<p><code class="highlighter-rouge"> 2&gt;filename </code>
  Redirects stderr to file “filename”.</p>

<p><code class="highlighter-rouge"> 2&gt;&gt;filename </code>
  Redirects and appends stderr to file “filename”.</p>

<p><code class="highlighter-rouge"> &amp;&gt;filename </code>
  Redirects both stdout and stderr to file “filename”.</p>

<p><code class="highlighter-rouge"> 2&gt;&amp;1 </code>
  Redirects stderr to stdout. Error messages get sent to same place as standard output.</p>
<hr />

<p>Some quality explanation now ;) Take the example of this command:
<br /><code class="highlighter-rouge"> cmd &gt;&gt; file.log 2&gt;&amp;1 </code>
<br />
This command will redirect all the output of command(cmd) into <code class="highlighter-rouge">file.log</code>.<br />
<code class="highlighter-rouge">2</code> refers to Second file descriptor of the process i.e., stderr<br />
<code class="highlighter-rouge">&gt;</code> refers to redirection<br />
<code class="highlighter-rouge">&amp;1</code> means that the target of redirection would be same as <code class="highlighter-rouge">1</code> i.e, first descriptor i.e, stdout.<br /></p>

<p>Thanks!</p>

<style type="text/css">
code {
    font-weight: bold;
    font-size: 18px;
    background: #ddd;
    padding: 3px;
}   
</style>

 ]]></content>
</entry>


  <entry>
  <title type="text">PostgreSQL - update timestamp when row(s) is updated</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2015/11/24/postgres-update-timestamp-when-row-is-updated.html" />
  <id>http://localhost:3000/blog/2015/11/24/postgres-update-timestamp-when-row-is-updated</id>
  <published>2015-11-24T00:00:00Z</published>
  <updated>2015-11-24T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>In PostgreSQL, if you want to set current timestamp as default value, you can simply keep a column’s default expression as <code class="highlighter-rouge">now()</code>. However, by default there is no function defined to update the timestamp when a particular row (or multiple rows) need to be updated.</p>

<p>In such scenario, you may create your custom method and trigger it using <b>PostgreSQL’s Triggers</b>. Following snippet will make it more clear:</p>

<p>Here, we are creating a new method, <code class="highlighter-rouge">method_get_updated_at()</code></p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">OR</span> <span class="k">REPLACE</span> <span class="k">FUNCTION</span> <span class="n">method_get_updated_at</span><span class="p">()</span> <span class="k">RETURNS</span> <span class="k">TRIGGER</span>
<span class="k">LANGUAGE</span> <span class="n">plpgsql</span>
<span class="k">AS</span> <span class="err">$$</span>
    <span class="k">BEGIN</span>
      <span class="k">NEW</span><span class="p">.</span><span class="o">&lt;</span><span class="k">column_name</span><span class="o">&gt;</span> <span class="o">=</span> <span class="n">now</span><span class="p">();</span>
      <span class="k">RETURN</span> <span class="k">NEW</span><span class="p">;</span>
    <span class="k">END</span><span class="p">;</span>
<span class="err">$$</span><span class="p">;</span></code></pre></figure>

<p>Once it is created, use the following snippet to trigger it:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TRIGGER</span> <span class="n">trigger_</span><span class="o">&lt;</span><span class="k">column_name</span><span class="o">&gt;</span>
<span class="k">BEFORE</span> <span class="k">UPDATE</span> <span class="k">ON</span> <span class="o">&lt;</span><span class="k">table_name</span><span class="o">&gt;</span>
<span class="k">FOR</span> <span class="k">EACH</span> <span class="k">ROW</span>
<span class="k">EXECUTE</span> <span class="k">PROCEDURE</span> <span class="n">method_get_updated_at</span><span class="p">();</span></code></pre></figure>

<p>If you want to delete a Trigger, you can use this simple query:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">DROP</span> <span class="k">TRIGGER</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">trigger_</span><span class="o">&lt;</span><span class="k">column_name</span><span class="o">&gt;</span> <span class="k">ON</span> <span class="o">&lt;</span><span class="k">table_name</span><span class="o">&gt;</span></code></pre></figure>

<p><b>Note:</b> Please update the <table_name> and <column_name> accordingly and execute the code for your particular database. Also, note that, some web frameworks (like Rails) manage such columns(created_at, updated_at) automatically.</column_name></table_name></p>

<p>ALso, if you want to view all existing methods, run this query:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span>  <span class="n">p</span><span class="p">.</span><span class="n">proname</span>
<span class="k">FROM</span>    <span class="n">pg_catalog</span><span class="p">.</span><span class="n">pg_namespace</span> <span class="n">n</span>
<span class="k">JOIN</span>    <span class="n">pg_catalog</span><span class="p">.</span><span class="n">pg_proc</span> <span class="n">p</span>
<span class="k">ON</span>      <span class="n">p</span><span class="p">.</span><span class="n">pronamespace</span> <span class="o">=</span> <span class="n">n</span><span class="p">.</span><span class="n">oid</span>
<span class="k">WHERE</span>   <span class="n">n</span><span class="p">.</span><span class="n">nspname</span> <span class="o">=</span> <span class="s1">'public'</span></code></pre></figure>

<p>And, run this query to view all Triggers:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">pg_trigger</span><span class="p">;</span></code></pre></figure>

<p>Thanks!</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">Accessing PostgreSQL server through a SSH Tunnel</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2015/11/20/accessing-postgreSQL-through-ssh-tunnel.html" />
  <id>http://localhost:3000/blog/2015/11/20/accessing-postgreSQL-through-ssh-tunnel</id>
  <published>2015-11-20T00:00:00Z</published>
  <updated>2015-11-20T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p><b>Step 1:</b> Check the SSH connectivity with the server, verify username and password.</p>

<p><b>Step 2:</b> Create the tunnel in your local system by executing the following command (It will prompt for password):</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">ssh <span class="nt">-fNg</span> <span class="nt">-L</span> 5555:localhost:5432 &lt;user&gt;@&lt;server&gt;</code></pre></figure>

<p><b>Step 3:</b> Now, open your PostgreSQL client (eg, <code class="highlighter-rouge">pgAdmin 3</code> or <code class="highlighter-rouge">DBeaver</code> or <code class="highlighter-rouge">Postico</code> for OS X or <code class="highlighter-rouge">Terminal</code>) and fill in the connection details as usual. Check the image below.</p>

<p><img class="img-responsive" src="http://localhost:3000/assets/images/postico-port-forwarding.png" alt="Postico DB connection" /></p>

<p><b>Note:</b> Yes, you’ll have to use <code class="highlighter-rouge">'localhost'</code>.</p>
 ]]></content>
</entry>



</feed>
