<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
    <title type="text" xml:lang="en">Shekhar Singh | All things Backend. Services, Data, DevOps | Consultant, Freelancer</title>
    <link type="application/atom+xml" rel="self" href="http://localhost:3000/atom.xml"/>
  
  <link href="http://localhost:3000/"/>
  <id>http://localhost:3000/</id>
  <updated>2019-08-26T20:04:18Z</updated>
  <author>
    <name>Shekhar Singh</name>
    <email>shekhar.singh@msn.com</email>
  </author>
  <rights type="text">Copyright ¬© 2019 Shekhar Singh. All rights reserved.</rights>
  
  <entry>
  <title type="text">Cycling Trip - Bangalore to Kolar/Avani village - 240 km in 11 hours</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2019/08/15/cycling-trip-bangalore-to-kolar-avani-240-km-in-11-hours.html" />
  <id>http://localhost:3000/blog/2019/08/15/cycling-trip-bangalore-to-kolar-avani-240-km-in-11-hours</id>
  <published>2019-08-15T00:00:00Z</published>
  <updated>2019-08-15T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p><b>Content:</b></p>
<ul>
	<li><a href="#section_1">Why this trip?</a></li>
	<li><a href="#section_2">Places &amp; temples I visited</a></li>
	<li><a href="#section_3">Hardest part: The return trip to the home</a></li>
</ul>

<p><span style="color: #ccd;"><i>Note: This post is slightly detailed. I wanted to keep it as a ‚ÄúNotes to self‚Äù; because I don‚Äôt want to forget some of the moments from this trip; especially the 3rd part of this post.</i></span></p>

<p id="section_1">
  It's been a couple of months, that I had gone for a long cycling trip. 15th August on a Thursday means long weekend was coming (kind of). On Wednesday, I was thinking about the places I can go; but I and my cycling partner <a href="https://twitter.com/nitishsp" target="_blank">Nitish</a> (we call him <i>Parkar</i>) had been to most Bangalore's good places in the range of around ~120 km round trip. Kolar's blue water lake (also called "Dodda" or "Chhota Ladakh") was in my mind from last few times which is roughly 52 km from Indiranagar. In the Bangalore-Sakleshpura-Chikmangaluru cycling trip (a post soon), Parkar and I had clocked our best of 135 km in a single day. I wanted to go farther than that from quite some time, but couldn't find a good destination within the range. More than that, a destination which could be safe to go; if I get tired and could crash somewhere midway.
</p>

<p>
  Therefore, I decided to visit Kolar and cover some of the nearby ancient temples that I had heard of. I created a map which included the lake and 6 temples (Targetting ~220 km). Parkar couldn't join me as he was still recovering from the viral fever. In the evening, I washed the bike, did the basic checks and packed up the small decathlon bag with all the utilities (checklist <a href="http://localhost:3000/blog/2019/06/11/long-cycling-ride-checklist-endurace-ride.html" target="_blank">here</a>). I quickly prepared my go-to mixture of roasted grams + raisins + almonds + peanuts (~500gm overall). I knew that nutrition and hydration are going to be the deciding factor. I slept at around 1 AM since Me/Parkar/Vishwas were chilling at my home; which already delayed my plan by an hour. I got up at 6 AM.
</p>

<p>
  In the morning, I had to visit Parkar's house first, to get bike's chain lube, since the bike was not serviced for a while. Then, I checked the map, plugged-in earphones, started a long playlist and set the goal to come back home by 7 PM; a total of 12.5 hours trip.
  I started cycling at good speed; so that while returning I could spare a few hours in case things go bad. I bought a small Indian flag üáÆüá≥ on the way (NH 75) since many people were selling it on the way throughout for Independence Day celebration. At this point, I realized I had forgotten to tie Rakhi :-( It was Rakshabandhan too.
</p>

<center><img class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/kolar_avani/dodda_blue_water_lake.jpg" alt="Kolar dodda blue water lake" style="width:600px;height:450px;" /></center>

<h3 id="section_2">Dodda ayur Chhota Ladakh</h3>
<p>
  In less than 2 hours, I had reached the Chhota Ladakh area. It was really fun so far, good songs and speed! My water bottle fell and broke due to a sudden speed breaker that I didn't see on the way. On the other hand, I couldn't find the lake for next ~30 minutes; since it was not correctly marked on the map, and no signs, etc. I moved in the wrong direction for about 2-3 km as well and then had to come back. Finally, reached the place after asking the only farmer I could see in that small village. The lake and rocks around were unexpectedly huge. The place was all empty, apart from a security guard (claimed by him); the strong winds made it a pretty chilling place.
</p>

<p><b>Kolar &amp; Avani</b> has a very rich history. Valmiki lived here, Lav &amp; Kush were born in Avani too. There are a large number of temples in this whole region. It‚Äôs very popular for Gold fields. <a href="https://en.wikipedia.org/wiki/Kolar" target="_blank">(wiki)</a></p>

<h3>Antharagange</h3>

<p>I had my breakfast bars, some dry fruits and I left for Kolar city, which was around 11 km from here. This was my fastest 11 km; which I completed in 17 mins; but again the location was marked incorrectly on Google Maps. I had to ride a few extra kms here and there to finally find it. This place is full of monkeys (actual monkeys, I mean) and it has trek starting through a bunch of staircases to many caves through the rocks. It‚Äôs a Lord Shiva‚Äôs temple, which means ‚ÄúThe Ganges from the deep‚Äù in Kannada. It‚Äôs also known as Kashi of South. <a href="https://en.wikipedia.org/wiki/Antara_Gange" target="_blank">(wiki)</a> I had two tender coconuts before I left from here.</p>

<center><img style="width:650px;height:600px;" class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/kolar_avani/antharagange.jpg" alt="Antara Gange" /></center>

<h3>Someshwara &amp; Kolaramma Temples</h3>
<p>Next two temples were within the Kolar city. Most of these temples were built by <a href="https://en.wikipedia.org/wiki/Chola_dynasty" target="_blank">Cholas</a> time. I covered them quickly and wanted to grab a breakfast too. Google Maps was showing that Kolar had a big lake, but when I reached the location, it was just a dried out huge lake with plastics, and trees inside it. Imagine, this state of the lake was during the monsoon.</p>

<div style="width: 750px; height: 390px;">
	<div style="width: 510px; height: 390px;float: left;">
		<img class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/kolar_avani/sri_somehwara.jpg" alt="Someshwara Temple" style="width:510px;" />	
	</div>
	<div style="width: 235px; height: 390px;float: left; margin-top: 9px;">
		<img class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/kolar_avani/kolaramma.jpg" alt="Kotilingeshwara Temple" style="height: 365px;" />
	</div>
</div>

<p>Anyway, I realized in the search of lake and a good Dosa place, I had come out of the city on a state highway (SH 96) which took me out of the city and back to NH 75. Also, I got a call from home and got scolded for not tying the rakhi yet. I borrowed a few hours from them. <u>While cycling, you can talk to people for long.</u> :)</p>

<p>I decided to have the proper breakfast after reaching my next stop, Avani Village, which was roughly 30 km from here. Plus, I have already been snacking on the items I got with me. I had a few glasses of sugarcane juice and I started for the next stop. Although, it‚Äôs not too tough to find restaurants on highways, sometimes it can get tough in remote areas; especially on a cycle.</p>

<p>Around 25 km later, I took a right-turn into Avani village, leaving the highway - which goes to Tirupathi. I saw a board saying it was 151 km from there, and it got me thinking if I should rather go Tirupathi, but maybe next time :) This road was better, huge trees on both sides and not wide like highways. Plus, my speed has been really good so far. I reached the next temple within 1 hour 12 mins. These villages were really small. I didn‚Äôt see a lot of people. Weather was okay, not too humid or too hot.</p>

<h3>Ramalingeshwara Temples</h3>
<p>These temples were a group of many temples. They were built before Chola dynasty (10th century), who had renovated these. There were barely 5-7 people around though. I roamed around, read some of the stories on the boards, and then took a nap for 15 minutes on a huge stone under a tree. I decided to leave for the next place by 1 PM. I ate most of the <a href="https://en.wikipedia.org/wiki/Chikki" target="_blank">chikki</a> I had, drank water and left for the next place at 1 PM.</p>
<div style="width: 750px; height: 600px;">
	<div style="width: 750px; height: 285px;float: left;">
		<div style="width: 370px; height: 300px;float: left;margin-right: 5px;">
			<img class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/kolar_avani/ramalingeshwara_1.jpg" alt="Kotilingeshwara Temple" style="width:390px;" />	
		</div>
		<div style="width: 370px; height: 300px;float: left;">
			<img class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/kolar_avani/ramalingeshwara_2.jpg" alt="Kotilingeshwara Temple" style="width:390px;" />	
		</div>
	</div>
	<div style="width: 750px; height: 300px;float: left;">
		<div style="width: 370px; height: 300px;float: left;margin-right: 5px;">
			<img class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/kolar_avani/ramalingeshwara_3.jpg" alt="Kotilingeshwara Temple" style="width:390px;" />	
		</div>
		<div style="width: 370px; height: 300px;float: left;">
			<img class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/kolar_avani/ramalingeshwara_4.jpg" alt="Kotilingeshwara Temple" style="width:390px;height: 278px;" />	
		</div>
	</div>
</div>

<p>These were really bad roads. For a stretch of over 8-10 km, I went through those patchy and broken roads. I was slightly worried about the cycle, since it has only hybrid tyres, and it‚Äôs not a mountain bike. These roads took me to some very remote areas, small villages, jungles, and very dry lands. I was constantly checking maps since there were a lot of turns and I didn‚Äôt want to take some bad route. Finally, I reached the next destination in a village.</p>

<h3>Kotilingeshwara Temple</h3>
<p>Unlike, all the above temples, this temple was super crowded. Suddenly, I could see a lot of humans around. This temple is not a very old temple and is also called mini-Tirupathi. It has the largest Shivling in the world and has 10 million+ small-sized Shivlings. It looked like a huge shop at first, but these Shivlings are actually donated by the devotees.</p>
<center><img style="width:500px;" class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/kolar_avani/kotilingeshwara.jpg" alt="Kotilingeshwara Temple" /></center>
<p>It was very crowded, and I set the time of 3 PM to start for my return trip. I had to skip the huge queue to go inside the main temple. Although they were offering free meals there, I decided to finally have my lunch outside in a local Dosa shop. Two Onion Dosas. Bought a water bottle and I left from here.
<br /></p>

<h3 id="section_3">Hardest part: The return trip to the Home</h3>
<p>I checked on the map; it was 90 km. I thought it shouldn‚Äôt take me more than 4.5 hours to reach home considering the trip so far. I‚Äôve been feeling pretty good; except for the bad road part and was not fully tired yet.</p>

<p>After around next 4-5 km, I realized that I was riding at a slower speed for some reason, plus I started getting tired. Soon, I understood that I was going against the direction of winds, which was added trouble on those roads with so many up-and-downs. I was surprised to face such high impact of winds. My next goal was to cut into NH 75 near Kolar, which was around 32 km. This stretch was the toughest. I drank a lot of water, ate some more carbs, in that jungle-ish and dry land road. It was SH-96. I couldn‚Äôt ride well. But I thought, once I get back to the highway, I will be able to get the speed. So my goal was to just reach Kolar. By the way, I had not still tied the Rakhi. <u>I could read the notifications in my watch's screen that my family's Whatsapp group was angry at me.</u></p>

<p>It took me <b>2.5 hours</b> to finish this stretch. This drained out all my energy and suddenly I started to have lower back pain, knees + legs pain, and palms were getting numb. The pain was expected even before I started the trip. In all my previous trips, I  never had pain in my lower legs, but this time maybe because of higher speed, it caught me. I put the pain relief spray, which eventually helped with my lower back pain after 30 minutes. But the legs were in bad shape.</p>

<p>After reaching Kolar on NH 75, I had 60 km to go. Unexpectedly, within 10 km, I felt I was riding really slow. The opposing wind was still there. I was worried that I might have to ride till very late in the night on this highway, which is not safe considering there are no street lights on most part of the highways. I wanted to reach the next major stop, Hoskote. which was 40 km from Kolar. I thought I was riding at a speed of 12-13 km/hour, which was really bad. Highways are un-even, there are too many ups-and-downs, flyovers, etc; which you realize only if you‚Äôre running or cycling. <b><i>I had given up at this point.</i></b> So I planned to take a lift till Hoskote from some truck or lorry. In my first attempt, one lorry stopped, but he said, he is going to take the next immediate left, which he actually took in 50 meters. That was slightly disappointing. I checked the maps for the distance and I realized, I was averaging exactly 20 kmph. This was good news, suddenly my calculations were on the safer side, and I felt slightly motivated. My phone battery was now 20%, so <u>I had to turn off the music at this point</u>. Only Google Maps was active now.</p>

<p><b>In my head, I made a plan</b> that I‚Äôll take a 5 minutes break, in every 30 minutes, and after every 10 km. Which means I will reach Bangalore by 7:30 PM and home before 9 PM. I broke that remaining 50 km into 5 chunks. Hoskote was 30 km from there.</p>

<p>My palms were aching and slightly numb. Legs were painful. But my plan was working. I was clocking exact 20 kmph. Meanwhile, It was getting dark, I kept dropping my location as checkpoints to Parkar/Vishwas in a Whatsapp group; and actually, asked them, <i>If I don‚Äôt call them by 9 PM, give me a call.</i> On the highway, I could see every kilometer getting reduced to Hoskote on those sideboards.</p>

<p><b>At this point, I had only the following things going in my head</b>: 1. Tie Rakhi when you reach home. 2. Cover next 10 km in 30 minutes. 3. Reach Hoskote. 4. Eminem‚Äôs <i>Till I Collapse</i> verse. I couldn‚Äôt literally think of anything else happening in my life, I couldn‚Äôt sing any songs in my head either. Usually, I love the time when I ain‚Äôt thinking about anything else. eg, during a workout, or some deep work. But this was different.</p>

<p>Soon, I could see more trucks and street lights, and therefore I knew Hoskote was close. I filled my pocket with all the remaining roasted grams + almonds etc mix I had and kept eating it. I had reached Bangalore‚Äôs outer area, with exactly as planned time. I was very happy at this point and my next goal was to reach home :) At this point, I couldn‚Äôt speak the whole Eminem‚Äôs verse in my head either: I could only say <i>Till I Collapse</i>, which sometimes I even shouted to keep myself active mentally.</p>

<p>But Bangalore‚Äôs traffic was as usual bad in the evening. I had to stop at many signals and a few traffic jams. Problem with stopping was that <i>it breaks the momentum and standing on the feet was more painful that pedaling the bicycle.</i> The last 10 km turned out to be much worse, because of traffic. I reached home at exact 8:30 PM.</p>

<p>I drank a few sips of water and crashed on the bed. I was feeling much weaker on the bed than on the cycle; maybe because I was home. I zoned out, and got up after 30 mins, I took shower, tied all 7 Rakhis and sent the photo to my family‚Äôs Whatsapp group. Two ORS, Eat Fit‚Äôs spinach soup and half-liter milk helped me feel much better after 2 hours.</p>

<p><br /></p>
<h4><i>
We can overcome any problem in life if we split the problem into small milestones/steps and just focus on one step at a time.
</i></h4>

<p><br />
Here‚Äôs the map of the trip:</p>

<center><img class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/kolar_avani/map.png" alt="Map" style="width:750px;height:220px;" /></center>
<!-- https://www.google.co.in/maps/dir/Indiranagar,+Bengaluru,+Karnataka/Dodda+ayur+Chhota+Ladakh,+Arabikothanur,+Karnataka+563133/Antharagange+Main+Road,+Kuvempu+Nagar,+Kolar,+Karnataka/Sri+Someshwara+Temple,+Fort+Area,+Kolar,+Karnataka/Kolaramma+Temple,+Fort+Area,+Kolar,+Karnataka/Ramalingeshwara+Temple,+Avani,+Karnataka/Shree+Kotilingeshwara+Swamy+Temple,+Road,+Ghattakamadenahalli,+Karnataka/Indiranagar,+Bengaluru,+Karnataka/@13.116783,77.9819646,11z/data=!4m50!4m49!1m5!1m1!1s0x3bae16a418770391:0xb50f46b826501036!2m2!1d77.6408356!2d12.9783692!1m5!1m1!1s0x3badfb7c8a1d3b19:0x89e9f0e21855fe44!2m2!1d78.0241684!2d13.1148468!1m5!1m1!1s0x3badf09d2efe6ebf:0xfb33139875963da!2m2!1d78.115183!2d13.1377839!1m5!1m1!1s0x3badff7368206e87:0xd564cfd017b0d933!2m2!1d78.13823!2d13.1374815!1m5!1m1!1s0x3badf0f22254065f:0xa579ad6d50b3e353!2m2!1d78.1391205!2d13.1384881!1m5!1m1!1s0x3bad8db3dd415165:0x805925a2adc42e9!2m2!1d78.3294587!2d13.1070527!1m5!1m1!1s0x3bad94ab7b67cf47:0x1528ee14bcc9a5be!2m2!1d78.2957015!2d12.9951555!1m5!1m1!1s0x3bae16a418770391:0xb50f46b826501036!2m2!1d77.6408356!2d12.9783692!3e0 -->
 ]]></content>
</entry>


  <entry>
  <title type="text">Checklist - Long cycling ride (endurance rides)</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2019/06/11/long-cycling-ride-checklist-endurace-ride.html" />
  <id>http://localhost:3000/blog/2019/06/11/long-cycling-ride-checklist-endurace-ride</id>
  <published>2019-06-11T00:00:00Z</published>
  <updated>2019-06-11T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>
Every time I plan for a cycling trip, while preparing I always think about if I'm missing anything. This post is to fix that. I'll add a similar checklist for running and hiking as well.
</p>

<center><img class="img-responsive" src="http://localhost:3000/assets/images/cycling_trip/cycling_checklist.jpg" alt="Kolar dodda blue water lake" style="width:750px;height:400px;" /></center>

<h4>Checklist:</h4>

<p><b>For Cycle:</b><br />
<input type="checkbox" /> Cycling Helmet (<a target="_blank" href="https://www.decathlon.in/p/8500051_st-50-mountain-bike-helmet-black.html">decathlon</a>)<br />
<input type="checkbox" /> Wrench Spanner (<a target="_blank" href="https://www.amazon.in/dp/B00LGE1Z">amazon</a>)<br />
<input type="checkbox" /> 2 Cycle tyre tubes (<a target="_blank" href="https://www.decathlon.in/p/8311095_700x1825-inner-tube-80mm-presta.html">decathlon</a>)<br />
<input type="checkbox" /> Tyre levers (<a target="_blank" href="https://www.decathlon.in/p/8359629_puncture-repair-kit-and-adjustments-500-hand-pump-3-tyre-levers-1-multitool.html">decathlon</a>)<br />
<input type="checkbox" /> Multitool (<a target="_blank" href="https://www.decathlon.in/p/8359629_puncture-repair-kit-and-adjustments-500-hand-pump-3-tyre-levers-1-multitool.html">decathlon</a>)<br />
<input type="checkbox" /> Cycle Hand Pump (<a target="_blank" href="https://www.decathlon.in/p/8359629_puncture-repair-kit-and-adjustments-500-hand-pump-3-tyre-levers-1-multitool.html">decathlon</a>)<br /></p>

<p><b>For you:</b><br />
<input type="checkbox" /> Cycling gloves (<a target="_blank" href="https://www.decathlon.in/p/8327103_300-cycling-gloves-black.html">decathlon</a>)<br />
<input type="checkbox" /> Cycle foam saddle (<a target="_blank" href="https://www.decathlon.in/p/8381098_500-memory-foam-saddle-cover-size-m-black.html">decathlon</a>)<br />
<input type="checkbox" /> Cycling sunglasses (<a target="_blank" href="https://www.decathlon.in/p/8118519_arenberg-sunglasses-cycling-running-adult-yellow-category-1.html">decathlon</a>)<br />
<input type="checkbox" /> Arm UV protection cover (<a target="_blank" href="https://www.decathlon.in/p/8518751_roadr-arm-cover-uv-black.html">decathlon</a>)<br />
<input type="checkbox" /> Face mask/Headband (<a target="_blank" href="https://www.decathlon.in/p/8493103_mountain-trekking-headband-trek-500-multi-position-black.html">decathlon</a>)<br />
<input type="checkbox" /> Raincoat (<a target="_blank" href="https://www.decathlon.in/p/8300326_rain-cut-men-s-rain-hiking-jacket-black.html">decathlon</a>)<br /></p>

<p><b>Others:</b><br />
<input type="checkbox" /> A small bag (waterproof if possible) (<a target="_blank" href="https://www.decathlon.in/p/8348925_arpenaz-10-ultra-compact-ultra-lightweight-backpack-blue.html">decathlon</a>)<br />
<input type="checkbox" /> Small Plastic to cover bag or carry cash<br />
<input type="checkbox" /> Some cash<br />
<input type="checkbox" /> ID proof<br />
<input type="checkbox" /> Emergency contact details<br />
<input type="checkbox" /> Power bank<br />
<input type="checkbox" /> Pain relief spray<br /></p>

<p><b>Nutrition:</b><br />
<input type="checkbox" /> Breakfast bars (Yoga bars etc)<br />
<input type="checkbox" /> My favourite: Roasted Grams + Almonds + Peanuts + Raisins mix<br />
<input type="checkbox" /> Chikki and other dry fruits<br />
<input type="checkbox" /> Water bottle(s)<br />
<input type="checkbox" /> ORS / Tang etc.<br /></p>

<p><br />
Note: Make sure to get the items as per your cycle‚Äôs size. These links are mostly where I got them from.</p>

<p>Add a comment if you think I‚Äôm missing anything.</p>

 ]]></content>
</entry>


  <entry>
  <title type="text">Analyzing Python Pandas' memory leak and the fix</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2019/03/26/analyzing-pandas-memory-leak-issue-with-fix.html" />
  <id>http://localhost:3000/blog/2019/03/26/analyzing-pandas-memory-leak-issue-with-fix</id>
  <published>2019-03-26T00:00:00Z</published>
  <updated>2019-03-26T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>At <a target="_blank" href="https://getsimpl.com">Simpl</a>, we use <a target="_blank" href="https://pandas.pydata.org/">pandas</a> heavily to run a bunch of our machine learning models many of them implemented with scikit-learn. We‚Äôve been growing rapidly and sometime back, one of the models crashed with python‚Äôs <code class="highlighter-rouge">MemoryError</code> exception. We were pretty sure that the hardware resources are enough to run the task.</p>

<p>What is the <b>MemoryError</b>? It‚Äôs an exception thrown by interpreter when not enough memory is available for creation of new python objects or for a running operation.</p>

<p>The catch here is that, it doesn‚Äôt necessarily mean ‚Äúnot enough memory available‚Äù. It could also mean that, there are some objects that are still not cleaned up by Garbage Cleaner (GC).</p>

<p>To test this, I wrote a very small script:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">arr</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000000</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">blast</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">xs</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">blast</span><span class="p">()</span>
</code></pre></div></div>

<p>Below is the distribution (Memory usage w.r.t Time) <u>before the program crashed</u> with MemoryError exception.</p>

<p><img class="img-responsive" src="http://localhost:3000/assets/images/pandas_memory_leak.png" alt="Pandas memory leak" /></p>

<p>The GC seems to be working fine, but it‚Äôs not able to clean up the objects as fast as it‚Äôs required in this case.
<br /><br /></p>

<p><b>What‚Äôs the issue?</b><br />
Python‚Äôs default implementation is <code class="highlighter-rouge">CPython</code> (<a target="_blank" href="https://github.com/python/cpython">github</a>) which is implemented in C. The problem was <a target="_blank" href="https://sourceware.org/bugzilla/show_bug.cgi?id=14827">this</a> bug; in the implementation of <code class="highlighter-rouge">malloc</code> in <code class="highlighter-rouge">glibc</code> (which is GNU‚Äôs implementation of C standard library).
<br /><br /></p>

<p><b>Issue Details</b>:<br />
<code class="highlighter-rouge">M_MXFAST</code> is the maximum size of a requested block that is served by using optimized memory containers called <code class="highlighter-rouge">fastbins</code>. <code class="highlighter-rouge">free()</code> is called when a memory cleanup of allocated space is required; which triggers the trimming of fastbins. Apparently, when <code class="highlighter-rouge">malloc()</code> is less than <code class="highlighter-rouge">M_MXFAST</code>, <code class="highlighter-rouge">free()</code> is not trimming fastbins. But, if we manually call <code class="highlighter-rouge">malloc_trim(0)</code> at that point, it should free() up those fastbins as well.</p>

<p>Here is a snippet from <code class="highlighter-rouge">malloc.c</code>‚Äôs <code class="highlighter-rouge">free()</code> implementation (alias <code class="highlighter-rouge">__libc_free</code>). (<a target="_blank" href="https://code.woboq.org/userspace/glibc/malloc/malloc.c.html#3115">link</a>)</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">p</span> <span class="o">=</span> <span class="n">mem2chunk</span> <span class="p">(</span><span class="n">mem</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">chunk_is_mmapped</span> <span class="p">(</span><span class="n">p</span><span class="p">))</span>                       <span class="cm">/* release mmapped memory. */</span>
    <span class="p">{</span>
      <span class="cm">/* See if the dynamic brk/mmap threshold needs adjusting.
         Dumped fake mmapped chunks do not affect the threshold.  */</span>
      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">mp_</span><span class="p">.</span><span class="n">no_dyn_threshold</span>
          <span class="o">&amp;&amp;</span> <span class="n">chunksize_nomask</span> <span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">mp_</span><span class="p">.</span><span class="n">mmap_threshold</span>
          <span class="o">&amp;&amp;</span> <span class="n">chunksize_nomask</span> <span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">DEFAULT_MMAP_THRESHOLD_MAX</span>
          <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">DUMPED_MAIN_ARENA_CHUNK</span> <span class="p">(</span><span class="n">p</span><span class="p">))</span>
        <span class="p">{</span>
          <span class="n">mp_</span><span class="p">.</span><span class="n">mmap_threshold</span> <span class="o">=</span> <span class="n">chunksize</span> <span class="p">(</span><span class="n">p</span><span class="p">);</span>
          <span class="n">mp_</span><span class="p">.</span><span class="n">trim_threshold</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">mp_</span><span class="p">.</span><span class="n">mmap_threshold</span><span class="p">;</span>
          <span class="n">LIBC_PROBE</span> <span class="p">(</span><span class="n">memory_mallopt_free_dyn_thresholds</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
                      <span class="n">mp_</span><span class="p">.</span><span class="n">mmap_threshold</span><span class="p">,</span> <span class="n">mp_</span><span class="p">.</span><span class="n">trim_threshold</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="n">munmap_chunk</span> <span class="p">(</span><span class="n">p</span><span class="p">);</span>
      <span class="k">return</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>Therefore, we need to trigger <code class="highlighter-rouge">malloc_trim(0)</code> from our python code written above; which we can easily do using <code class="highlighter-rouge">ctypes</code> module.
<br /><br />
The fixed implementation looks like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">ctypes</span> <span class="kn">import</span> <span class="n">cdll</span><span class="p">,</span> <span class="n">CDLL</span>
<span class="n">cdll</span><span class="o">.</span><span class="n">LoadLibrary</span><span class="p">(</span><span class="s">"libc.so.6"</span><span class="p">)</span>
<span class="n">libc</span> <span class="o">=</span> <span class="n">CDLL</span><span class="p">(</span><span class="s">"libc.so.6"</span><span class="p">)</span>
<span class="n">libc</span><span class="o">.</span><span class="n">malloc_trim</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">arr</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000000</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">blast</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">xs</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
        <span class="n">libc</span><span class="o">.</span><span class="n">malloc_trim</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">blast</span><span class="p">()</span>
</code></pre></div></div>
<p><br />
In another solution, I tried forcing the GC using python‚Äôs <code class="highlighter-rouge">gc</code> module; which gave the results similar to above method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gc</span>

<span class="n">arr</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000000</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">blast</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">xs</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span> <span class="c"># Forced GC</span>

<span class="n">blast</span><span class="p">()</span>
</code></pre></div></div>
<p><br />
The distrubution of Memory usage w.r.t Time looked much better now, and there was almost no difference in execution time. (see the ‚Äúdark blue‚Äù line)</p>
<p><img class="img-responsive" src="http://localhost:3000/assets/images/pandas_memory_leak_fix.png" alt="Pandas memory leak with fix" /></p>

<p><br /><br />
Similar cases, References and other notes:</p>
<ol>
  <li>Even after doing <code class="highlighter-rouge">low_memory=False</code> while reading a CSV using <code class="highlighter-rouge">pandas.read_csv</code>, it crashes with <code class="highlighter-rouge">MemoryError</code> exception, even though the CSV is not bigger than the RAM.</li>
  <li>Explanation of malloc(), calloc(), free(), realloc() deserves a separate post altogether. I‚Äôll post that soon.</li>
  <li>Similar reported issues:<br />
     - https://github.com/pandas-dev/pandas/issues/2659<br />
     - https://github.com/pandas-dev/pandas/issues/21353<br /></li>
</ol>

 ]]></content>
</entry>


  <entry>
  <title type="text">Pyspark&amp;#58; Why you should write UDFs in Scala/Java</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2018/08/17/spark-udf-in-scala-java-for-pyspark.html" />
  <id>http://localhost:3000/blog/2018/08/17/spark-udf-in-scala-java-for-pyspark</id>
  <published>2018-08-17T00:00:00Z</published>
  <updated>2018-08-17T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>While writing Pyspark programs, we would generally write UDFs in Python, which is a very obvious thing to do. However, the performance of Python UDFs are not as good as those written in Scala/Java.</p>

<p>Spark is written in Scala and Data/objects of a Spark program are stored in JVM. Pyspark API is just a wrapper over SparkSession, RDDs/DataFrame and other JVM objects (a few parts are in native python as well). This means that a Pyspark program goes through serialization and deserialization of JVM objects and data. This back and forth conversion affects the performance of Pyspark program drastically. Pyspark UDFs are a good example where this conversion happens a lot.</p>

<p><br />
<b>Example Implementation</b>:</p>

<p>Here is a very simple Java UDF to count length of a string:</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">package</span> <span class="n">com</span><span class="o">.</span><span class="na">rootcss</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.sql.api.java.UDF1</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">SparkJavaUdfExample</span> <span class="kd">implements</span> <span class="n">UDF1</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span class="o">(</span><span class="n">String</span> <span class="n">input</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="n">input</span><span class="o">.</span><span class="na">length</span><span class="o">();</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>
<p>(full Java code is available here: https://github.com/rootcss/PysparkJavaUdfExample)
<br /><br /></p>

<p>Once compiled, you can start pyspark by including the jar in the path.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bin/pyspark <span class="nt">--jars</span> /path/to/target/SparkJavaUdfExample-1.0-SNAPSHOT.jar
</code></pre></div></div>
<p><br /></p>

<p>Here‚Äôs the usage in Pyspark:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> from pyspark.sql.types import IntegerType
<span class="o">&gt;&gt;&gt;</span> <span class="c"># Now, register the UDF in Pyspark</span>
<span class="o">&gt;&gt;&gt;</span> sqlContext.registerJavaFunction<span class="o">(</span><span class="s2">"myCustomUdf"</span>, <span class="s2">"com.rootcss.SparkJavaUdfExample"</span>, IntegerType<span class="o">())</span>
<span class="o">&gt;&gt;&gt;</span> spark.sql<span class="o">(</span><span class="s2">"SELECT myCustomUdf('shekhar')"</span><span class="o">)</span>.collect<span class="o">()</span>
+------------+
|UDF<span class="o">(</span>shekhar<span class="o">)</span>|
+------------+
|           7|
+------------+
</code></pre></div></div>

<p><br />
<b>References</b>:</p>
<ul>
  <li>Slide #59 in https://0x0fff.com/wp-content/uploads/2015/11/Spark-Architecture-JD-Kiev-v04.pdf</li>
  <li>This post covers this whole topic in more details - https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9</li>
</ul>

 ]]></content>
</entry>


  <entry>
  <title type="text">Using Cloudflare's SSL for your website (FREE!)</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2018/08/16/using-cloudflare-s-ssl-for-your-website-free.html" />
  <id>http://localhost:3000/blog/2018/08/16/using-cloudflare-s-ssl-for-your-website-free</id>
  <published>2018-08-16T00:00:00Z</published>
  <updated>2018-08-16T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>Many people who have static websites/blogs would say, they don‚Äôt really need SSL for their website and I‚Äôve said that too in past. SSL certificates are not cheap comparatively and we try to save that additional cost.</p>

<p>Internet Providers have been playing around with the web traffic and tampering it for a while now. Sniffing the traffic and content injection in web pages is common. These are some simple examples for why SSL is important.</p>

<p>Coming to the point, <a target="_blank" href="https://www.cloudflare.com/">Cloudflare</a> provides free SSL for your website, and it‚Äôs super-easy to configure:</p>
<ul>
  <li>Sign up on Cloudflare and enter your DNS. After that, they will automatically fetch all the DNS Records associated with your domain. You can cross-check from your domain provider to be sure.</li>
  <li>In the next step, they will provide few (usually 2) new Nameservers (something like blah.ns.cloudflare.com)</li>
  <li>You just need to replace the existing Nameservers with these new ones in your Domain provider‚Äôs dashboard.</li>
  <li>Wait for DNS resolution to complete. Meanwhile, go to Cloudflare dashboard‚Äôs Crypto menu, and Turn ON <b>Always use HTTPS</b> option.</li>
  <li>Once you see <b>Universal SSL Status: Active Certificate</b> in the dashboard, your website will have working SSL. (happend for me within 5 minutes; but might take upto 24 hours)</li>
</ul>

<p><br />
<b>How does it work?</b></p>

<p>Cloudflare has three ways in which you can use SSL.</p>
<ul>
  <li><b>Flexible SSL</b>: Traffic between User &amp; Cloudflare is over HTTPS, but only HTTP between Cloudflare and your server.</li>
  <li><b>Full SSL</b>: If your server is configured with SSL, then the traffic is end to end over HTTPS. In my case, my website is hosted on Github Pages, which is configured with SSL, so I chose this option.</li>
  <li><b>Full SSL (Strict)</b>: Same as Full SSL, but additionally, the connection between Cloudflare &amp; web server is authenticated for every request as well. This needs a signed certificate from a trusted authority.</li>
</ul>

<p>You can <a target="_blank" href="https://support.cloudflare.com/hc/en-us/articles/200170416">read more</a> about it in Cloudflare‚Äôs official documentation.</p>

<p>Cloudflare also provides a lot of other useful features like Caching, Firewall to name a few.</p>

<p>PS: I got to know about Cloudflare‚Äôs free SSL plan today. Thank you Cloudflare.</p>

<p><b>References:</b></p>
<ul>
  <li>https://support.cloudflare.com/hc/en-us/articles/200170416</li>
  <li>https://www.cloudflare.com/</li>
</ul>
 ]]></content>
</entry>


  <entry>
  <title type="text">Configuring SSL step by step (and Amazon Certificate Manager)</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2018/08/15/configuring-ssl-step-by-step-and-amazon-certificate-manager.html" />
  <id>http://localhost:3000/blog/2018/08/15/configuring-ssl-step-by-step-and-amazon-certificate-manager</id>
  <published>2018-08-15T00:00:00Z</published>
  <updated>2018-08-15T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>Configuring SSL for your domains is still not as simple as it can be. Everytime I do that, I need to refer to my previous notes. Recently, I was using <a href="https://aws.amazon.com/certificate-manager/" target="_blank"><code class="highlighter-rouge">AWS Certificate Manager</code></a> to setup a PositiveSSL Wildcard certificate, so I thought of putting up my notes on the blog.</p>

<p>Note: This post focuses on configuring SSL, and very less on details about what &amp; why.</p>

<p><b>Step 1: Generate Certificate Signing Request (CSR) and Private key</b></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>openssl req <span class="nt">-new</span> <span class="nt">-newkey</span> rsa:2048 <span class="nt">-nodes</span> <span class="nt">-keyout</span> mydomain.key <span class="nt">-out</span> mydomain.csr
</code></pre></div></div>

<p>Enter the details like country, state etc when asked. After this step you will have two files:</p>

<ul>
  <li>mydomain.csr (the CSR file)</li>
  <li>mydomain.key (the private key)</li>
</ul>

<p>Content of these files look like following:</p>

<p>mydomain.csr</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----BEGIN CERTIFICATE REQUEST-----
:
-----END CERTIFICATE REQUEST-----
</code></pre></div></div>

<p>mydomain.key</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----BEGIN PRIVATE KEY-----
:
-----END PRIVATE KEY-----
</code></pre></div></div>

<p><br />
<b>Step 2: Buying the certificate from providers</b></p>

<p>When requesting for your SSL certificate on the providers like namecheap, godaddy etc, you‚Äôll be asked to enter your CSR content. Once you complete all the steps, you‚Äôll receive following files from the provider (I‚Äôm taking the example of files from Namecheap where I purchased the  certificate)</p>

<ol>
  <li>AddTrustExternalCARoot.crt [Root CA Certificate]</li>
  <li>COMODORSAAddTrustCA.crt [Intermediate CA Certificate]</li>
  <li>COMODORSADomainValidationSecureServerCA.crt [Intermediate CA Certificate]</li>
  <li>STAR_mydomain.crt [Your PositiveSSL Certificate]</li>
</ol>

<p><br />
<b>Step 3: Creating SSL Bundle</b></p>

<p>Using the files mentioned in Step 2, we‚Äôll be creating a SSL bundle, which is very simple. Just concatenate the content of first three files in right order as mentioned in the command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat </span>COMODORSADomainValidationSecureServerCA.crt COMODORSAAddTrustCA.crt AddTrustExternalCARoot.crt <span class="o">&gt;</span> ssl-bundle.crt
</code></pre></div></div>

<p>If you want to configure it for NGINX, then you need to concatenate your PositiveSSL certificate as well.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat </span>STAR_mydomain.crt COMODORSADomainValidationSecureServerCA.crt COMODORSAAddTrustCA.crt AddTrustExternalCARoot.crt <span class="o">&gt;</span> ssl-bundle.crt
</code></pre></div></div>

<p>Content of the bundle file will look something like this: (for ACM, only three entries will be there)</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----BEGIN CERTIFICATE-----
: - STAR_mydomain.crt
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
: - COMODORSADomainValidationSecureServerCA.crt
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
: - COMODORSAAddTrustCA.crt
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
: - AddTrustExternalCARoot.crt
-----END CERTIFICATE-----
</code></pre></div></div>

<p>Note:</p>
<ul>
  <li>The order of above files is very important.</li>
  <li>There shouldn‚Äôt be empty lines or line break between the certificates.</li>
</ul>

<p><br />
<b>Step 4: Configuring on NGINX</b></p>

<p>If you wanted to configure on NGINX, you just need these two files:</p>
<ul>
  <li>ssl-bundle.crt</li>
  <li>mydomain.key</li>
</ul>

<p>In you NGINX configuration, point these parameters to right path, restart NGINX and you‚Äôre good to go:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssl_certificate /etc/nginx/ssl/mydomain/ssl-bundle.crt;
ssl_certificate_key /etc/nginx/ssl/mydomain/mydomain.key;
</code></pre></div></div>

<p><br />
<b>Step 5: Configuring on Amazon Certificate Manager (ACM)</b></p>

<p>To configure it with ACM, you need to go through couple of more steps, as they required the certificates to be in certain format.
First convert your private key to .pem format:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>openssl rsa <span class="nt">-in</span> mydomain.key <span class="nt">-text</span> <span class="o">&gt;</span> mydomain-private-key.pem
</code></pre></div></div>

<p>Content:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----BEGIN RSA PRIVATE KEY-----
:
-----END RSA PRIVATE KEY-----
</code></pre></div></div>

<p>At last, to upload these certificates on ACM, use the following command:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws acm import-certificate <span class="se">\</span>
	<span class="nt">--certificate</span> file:///Users/rootcss/Downloads/ssl/STAR_mydomain.crt <span class="se">\</span>
	<span class="nt">--private-key</span> file:///Users/rootcss/Downloads/ssl/mydomain-private-key.pem <span class="se">\</span>
	<span class="nt">--certificate-chain</span> file:///Users/rootcss/Downloads/ssl/ssl-bundle.crt
</code></pre></div></div>

<p><br />
<b>Other notes:</b></p>

<ol>
  <li>If your private key is of format .pfx, you can convert it to .pem format directly using the command:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>openssl pkcs12 <span class="nt">-in</span> mydomain-private-key.pfx <span class="nt">-out</span> mydomain-private-key.pem <span class="nt">-nodes</span>
</code></pre></div>    </div>
    <p>(Enter the created password when asked)</p>
  </li>
  <li>If you need to convert your CSR file into PEM format, you can use the command:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>openssl x509 <span class="nt">-inform</span> PEM <span class="nt">-in</span> ssl-bundle.crt <span class="o">&gt;</span> ssl-bundle.crt
</code></pre></div>    </div>
  </li>
</ol>

<p><br />
<b>References:</b></p>
<ul>
  <li>https://aws.amazon.com/certificate-manager/</li>
  <li>http://nginx.org/en/docs/http/configuring_https_servers.html</li>
  <li>https://stackoverflow.com/questions/991758/how-to-get-pem-file-from-key-and-crt-files</li>
</ul>
 ]]></content>
</entry>


  <entry>
  <title type="text">Observations on querying Cassandra on 'multiple' partitions (with/without Spark)</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2017/04/01/querying-cassandra-from-spark-on-multiple-paritions.html" />
  <id>http://localhost:3000/blog/2017/04/01/querying-cassandra-from-spark-on-multiple-paritions</id>
  <published>2017-04-01T00:00:00Z</published>
  <updated>2017-04-01T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>Cassandra‚Äôs brilliancy totally depends on your data models. You should know beforehand about how the data will be accessed/queried; and then design accordingly.</p>

<p>If you‚Äôre querying a Cassandra table, you are going to start writing your query with the <b>partition key</b>, because as we know, the partition key tells about the data locality in the cluster. Writing a query that includes multiple partition keys is never optimized, because those keys might be on different nodes. Just assume, you have 500 nodes with RF=3 and each node is being scanned for those partition keys.</p>

<p>It‚Äôs going to be super expensive.</p>

<p>For example, Let‚Äôs say, I have a <code class="highlighter-rouge">users</code> table like this:</p>

<p>(partition key - id, clustering key - event_timestamp)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>id | event_timestamp | city
1  | abc............ | A
1  | def............ | B
2  | abc............ | B
3  | abc............ | C
:
:
</code></pre></div></div>

<p>Now, If I write a query like:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>This is perfectly optimized, and thanks to our Murmur3 partitioner we will get the result instantly.</p>

<p><br />
However, if I write a query like:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="k">IN</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">123</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">457</span><span class="p">,</span> <span class="mi">58</span><span class="p">,</span> <span class="mi">768</span><span class="p">,</span> <span class="mi">5435</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">547</span><span class="p">,</span> <span class="mi">456</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">2342</span><span class="p">,</span> <span class="mi">34</span><span class="p">....)</span>
</code></pre></div></div>

<p>On a small cluster this will cause no major issues, but on a 500 nodes cluster, it‚Äôs going to affect the JVM‚Äôs Heap badly, as explained above.</p>

<p><br />
Now, coming to <b>Spark</b>.</p>

<p>On a small scale, you wouldn‚Äôt even notice the problem. Not just with Spark, but even with CQLSH you wouldn‚Äôt notice the delay and issues significantly.
However, If your cluster is significantly large, it will be very slow and highly unoptimized, and we don‚Äôt really like that, right.</p>

<p><a href="https://github.com/datastax/spark-cassandra-connector" target="_blank"><code class="highlighter-rouge">cassandra-spark-connector</code></a> has a method called <code class="highlighter-rouge">joinWithCassandraTable()</code> to which you can pass a list of partition keys to be looked up.</p>

<p>Internally, this method extracts all the partition keys from the list, and runs a separate parallel query (spark tasks) for each partition key on our ‚Äúdistributed‚Äù Spark cluster (it uses Cassandra Java driver to perform this operation). Finally returns an RDD object consisting of results from all tasks.</p>

<p>So, our 2nd query was converted into something like this,</p>
<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">4</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">123</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">25</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">users</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">345</span>
<span class="p">:</span>
</code></pre></div></div>

<p>Usage of the method:</p>
<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="n">myList</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">partition_keys</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="nc">Tuple1</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>
<span class="k">val</span> <span class="n">myResult</span> <span class="k">=</span> <span class="n">myList</span><span class="o">.</span><span class="n">joinWithCassandraTable</span><span class="o">(</span><span class="n">keyspace</span><span class="o">,</span> <span class="s">"users"</span><span class="o">)</span>
</code></pre></div></div>

<p>We cannot say this is an extremely optimized solution, but considering the huge number of advantages that we get from Cassandra, we can compromise a bit here ;-)</p>

<p>And by the way, this method is not yet available for Pyspark, only in Scala. I am attempting to write one for Pyspark, will be sharing the details soon.</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">Spark Shell for Processing &amp;amp; Querying data in Cassandra</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2017/01/24/processing-cassandra-data-with-apache-spark-part-2.html" />
  <id>http://localhost:3000/blog/2017/01/24/processing-cassandra-data-with-apache-spark-part-2</id>
  <published>2017-01-24T00:00:00Z</published>
  <updated>2017-01-24T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>My previous <a href="processing-cassandra-data-with-apache-spark.html">post</a>  explains, how can you write a Spark job and execute it. In this post, I am writing down steps to initiate a Spark shell (pyspark or spark-shell), with a pre-established connection to Cassandra. In addition to this, I‚Äôll write down some sample codes and their outputs, in order to show the usage of Spark Transformations/Actions.</p>

<p>To start the shell, just run this command on your shell.
<br /></p>
<pre>
pyspark \
      --packages com.datastax.spark:spark-cassandra-connector_2.10:1.5.0-M2 \
      --conf spark.cassandra.connection.host=192.168.56.101 \
      --conf spark.cassandra.auth.username=cassandra \
      --conf spark.cassandra.auth.password=cassandra
</pre>

<p>I think, most of the parameters are pretty intuitive. In short, we are just providing the dependency packages and cassandra connection configurations. Make sure you provide path to <code class="highlighter-rouge">pyspark</code> or add it in your <code class="highlighter-rouge">$PATH</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Python 2.7.11 <span class="o">(</span>default, Nov 10 2016, 03:37:47<span class="o">)</span>
<span class="o">[</span>GCC 4.2.1 Compatible Apple LLVM 8.0.0 <span class="o">(</span>clang-800.0.42.1<span class="o">)]</span> on darwin

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _<span class="se">\ \/</span> _ <span class="se">\/</span> _ <span class="sb">`</span>/ __/  <span class="s1">'_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Python version 2.7.11 (default, Nov 10 2016 03:37:47)
SparkContext available as sc, HiveContext available as sqlContext.


&gt;&gt;&gt; sqlContext.sql("""CREATE TEMPORARY TABLE roles \
...                   USING org.apache.spark.sql.cassandra \
...                   OPTIONS ( table "roles", \
...                             keyspace "system_auth", \
...                             cluster "rootCSSCluster", \
...                             pushdown "true") \
...               """)
DataFrame[]


&gt;&gt;&gt; sqlContext.sql('</span>SELECT <span class="k">*</span> from roles<span class="s1">').show()
+---------+---------+------------+---------+--------------------+
|     role|can_login|is_superuser|member_of|         salted_hash|
+---------+---------+------------+---------+--------------------+
|cassandra|     true|        true|       []|$2a$10$pQW3iGSC.m...|
+---------+---------+------------+---------+--------------------+


&gt;&gt;&gt; sqlContext.sql("""CREATE TEMPORARY TABLE compaction_history \
...                   USING org.apache.spark.sql.cassandra \
...                   OPTIONS ( table "compaction_history", \
...                             keyspace "system", \
...                             cluster "rootCSSCluster", \
...                             pushdown "true") \
...               """)
DataFrame[]


&gt;&gt;&gt; sqlContext.sql('</span>SELECT <span class="k">*</span> FROM compaction_history LIMIT 3<span class="s1">').show()
+--------------------+--------+---------+-----------------+--------------------+-------------+
|                  id|bytes_in|bytes_out|columnfamily_name|        compacted_at|keyspace_name|
+--------------------+--------+---------+-----------------+--------------------+-------------+
|b54ccf00-e236-11e...|   20729|     4301|   size_estimates|2017-01-24 18:42:...|       system|
|170ddba0-e23f-11e...|   12481|     3085| sstable_activity|2017-01-24 19:42:...|       system|
|914d2e70-e195-11e...|      41|        0|  schema_triggers|2017-01-23 23:28:...|       system|
+--------------------+--------+---------+-----------------+--------------------+-------------+


&gt;&gt;&gt; sqlContext.sql('</span>SELECT columnfamily_name, COUNT<span class="o">(</span><span class="k">*</span><span class="o">)</span> AS count FROM compaction_history GROUP BY columnfamily_name<span class="s1">').show()
+-----------------+-----+
|columnfamily_name|count|
+-----------------+-----+
| schema_functions|    2|
|schema_aggregates|    2|
|  schema_triggers|    2|
| schema_usertypes|    2|
|   size_estimates|    2|
| sstable_activity|    4|
+-----------------+-----+


&gt;&gt;&gt; sqlContext.sql('</span>SELECT SUM<span class="o">(</span>bytes_in<span class="o">)</span> AS <span class="nb">sum </span>FROM compaction_history<span class="s1">').show()
+-----+
|  sum|
+-----+
|73610|
+-----+


&gt;&gt;&gt; df_payload = sqlContext.sql('</span>SELECT bytes_in, bytes_out, columnfamily_name FROM compaction_history<span class="s1">')
&gt;&gt;&gt; df_payload.count()
14


# Simple Map Reduce example
&gt;&gt;&gt; df_payload\
...      .map(lambda x: (x['</span>columnfamily_name<span class="s1">'], 1))\
...      .reduceByKey(lambda x, y: x+y)\
...      .toDF()\
...      .show()
+-----------------+---+
|               _1| _2|
+-----------------+---+
|   size_estimates|  2|
| sstable_activity|  4|
| schema_usertypes|  2|
|schema_aggregates|  2|
| schema_functions|  2|
|  schema_triggers|  2|
+-----------------+---+

&gt;&gt;&gt;

</span></code></pre></div></div>

<p><br />
I‚Äôll be adding more examples with time. :)</p>

<style>
pre code{
  white-space: pre;
}
</style>

 ]]></content>
</entry>


  <entry>
  <title type="text">Processing &amp;amp; Querying data in Cassandra with Apache Spark</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2017/01/23/processing-cassandra-data-with-apache-spark.html" />
  <id>http://localhost:3000/blog/2017/01/23/processing-cassandra-data-with-apache-spark</id>
  <published>2017-01-23T00:00:00Z</published>
  <updated>2017-01-23T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>This post is one of my <a href="new.html">Notes to Self</a> one. I‚Äôm simply going to write, how can you connect to Cassandra from Spark, run ‚ÄúSQL‚Äù queries and perform analysis on Cassandra‚Äôs data.</p>

<p><br />
Let‚Äôs get started.</p>

<p><br />
(Platform: Spark v1.6.0, Cassandra v2.7, macOS 10.12.1, Scala 2.11.7)</p>

<p>I‚Äôm going to use the package <code class="highlighter-rouge">spark-cassandra-connector</code> written by awesome <a href="http://www.datastax.com/">Datastax</a> guys.</p>

<p>Assuming you have already configured Cassandra &amp; Spark, it‚Äôs time to start writing a small Spark job.</p>

<p><b>Code with explanation</b>:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># imports necessary methods</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">SparkConf</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SQLContext</span>


<span class="c"># setup spark configuration object</span>
<span class="c"># this contains your cassandra connection parameters</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span>\
    <span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="s">"PySpark Cassandra"</span><span class="p">)</span> \
    <span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.cassandra.connection.host"</span><span class="p">,</span> <span class="s">"192.168.56.101"</span><span class="p">)</span>\
    <span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.cassandra.auth.username"</span><span class="p">,</span> <span class="s">"cassandra"</span><span class="p">)</span>\
    <span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.cassandra.auth.password"</span><span class="p">,</span> <span class="s">"cassandra"</span><span class="p">)</span>


<span class="c"># creates Spark Context with your cassandra configurations</span>
<span class="c"># local[*] represents that spark is going to use all cores of CPU for this job</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="s">"local[*]"</span><span class="p">,</span> <span class="s">"PySpark Cassandra"</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>


<span class="c"># creates Spark's sqlContext. This is going to be super useful.</span>
<span class="n">sqlContext</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>


<span class="c"># creates mapping with tables inside Cassandra for Spark</span>
<span class="c"># I am going to use "system_auth.roles" table here</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"""CREATE TEMPORARY TABLE roles </span><span class="se">\
</span><span class="s">                  USING org.apache.spark.sql.cassandra </span><span class="se">\
</span><span class="s">                  OPTIONS ( table "roles", </span><span class="se">\
</span><span class="s">                            keyspace "system_auth", </span><span class="se">\
</span><span class="s">                            cluster "rootCSSCluster", </span><span class="se">\
</span><span class="s">                            pushdown "true") </span><span class="se">\
</span><span class="s">              """</span><span class="p">)</span>

<span class="c"># you can create multiple mappings/temporary tables and write queries on it.</span>
<span class="c"># this in another table: "system.compaction_history"</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"""CREATE TEMPORARY TABLE compaction_history </span><span class="se">\
</span><span class="s">                  USING org.apache.spark.sql.cassandra </span><span class="se">\
</span><span class="s">                  OPTIONS ( table "compaction_history", </span><span class="se">\
</span><span class="s">                            keyspace "system", </span><span class="se">\
</span><span class="s">                            cluster "rootCSSCluster", </span><span class="se">\
</span><span class="s">                            pushdown "true") </span><span class="se">\
</span><span class="s">              """</span><span class="p">)</span>


<span class="c"># here's the query we are going to run</span>
<span class="n">query</span> <span class="o">=</span> <span class="s">"SELECT * FROM roles"</span>

<span class="k">print</span> <span class="s">"[Spark] Executing query: </span><span class="si">%</span><span class="s">s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="c"># The result of the query returns a dataframe</span>
<span class="n">df_payload</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="c"># priting the content of dataframe</span>
<span class="n">df_payload</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p><br /></p>

<p><b>Spark job Execution</b>:</p>

<p>To run your spark job, use the command below:</p>
<pre>
spark-submit --packages com.datastax.spark:spark-cassandra-connector_2.10:1.5.0-M2 myfile.py
</pre>

<p>(Note: Check <code class="highlighter-rouge">localhost:4040</code> in your browser for Spark UI)</p>

<p>--packages : This parameter tells Spark to download the external dependencies for the job.</p>

<p>In our case, we are using <code class="highlighter-rouge">spark-cassandra-connector</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>groupId: com.datastax.spark
artifactId: spark-cassandra-connector_2.10
version: 1.5.0-M2
</code></pre></div></div>

<p><br /></p>

<p><b>Output</b>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;all your logs will be printed here. including ivy logs&gt;
:
:
<span class="o">[</span>Spark] Executing query: <span class="k">select</span> <span class="k">*</span> from roles
+---------+---------+------------+---------+--------------------+
|     role|can_login|is_superuser|member_of|         salted_hash|
+---------+---------+------------+---------+--------------------+
|cassandra|     <span class="nb">true</span>|        <span class="nb">true</span>|       <span class="o">[]</span>|<span class="nv">$2a$10$pQW3iGSC</span>.m...|
+---------+---------+------------+---------+--------------------+
</code></pre></div></div>

<p><br />
Here, <code class="highlighter-rouge">df_payload</code> is DataFrame object. You can use all Spark‚Äôs <i>Transformations</i> &amp; <i>Actions</i> on this. (Check <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">here</a> for more details)</p>

<p><br />
<b>Second Part</b>: <u>Starting a Spark shell with Cassandra connection</u>
<br />
Steps for this is part of separate <a href="processing-cassandra-data-with-apache-spark-part-2.html">post</a>.</p>

<hr />

<p><b>Useful Links </b>:-</p>

<ol>
  <li>
    <p>I really want to thank guys at <a href="http://www.datastax.com/">Datastax</a>. They have written and open sourced, so many packages and drivers for Cassandra.</p>
  </li>
  <li>
    <p>You can contribute to <code class="highlighter-rouge">spark-cassandra-connector</code> <a href="https://github.com/datastax/spark-cassandra-connector">here</a>.</p>
  </li>
  <li>
    <p><a href="https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10/1.5.0-M2">Link</a> to <code class="highlighter-rouge">spark-cassandra-connector</code> maven repository.</p>
  </li>
</ol>
 ]]></content>
</entry>


  <entry>
  <title type="text">Processing Rabbitmq's Stream with &quot;Apache Flink&quot;</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000//blog/2016/11/12/apache-flink-rabbimq-streams-processor.html" />
  <id>http://localhost:3000/blog/2016/11/12/apache-flink-rabbimq-streams-processor</id>
  <published>2016-11-12T00:00:00Z</published>
  <updated>2016-11-12T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>I love <a target="_blank" href="http://spark.apache.org/">Apache Spark</a>. Not just becacuse of it‚Äôs capability to adapt to so many use-cases, but because it‚Äôs one of shining star in the <i>Distributing Computing</i> world, has a great design and superb community backing.</p>

<p>However, one of the features I‚Äôd want enhancement in is, the way it processes the streams. Spark processes the streams in a <b>micro-batch</b> manner i.e, you set a time interval (could be any value), and Spark will process the events collected in that interval, in batch. This is where Apache Flink comes in!</p>

<p><a target="_blank" href="http://spark.apache.org/">Apache Flink</a> is often comapred with Spark. I feel Spark is far ahead of Flink, not just in technology; but even community backing of Spark is very big, compared to Flink.</p>

<p>Anyways, this post is not about comparing them, but to provide a detailed example of processing a RabbitMQ‚Äôs stream using Apache Flink.</p>

<p><b>Step 1:</b> Install Rabbitmq, Apache Flink in your system. Both installations are very straightforward.</p>

<p><b>Step 2:</b> Start Rabbitmq server</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rabbitmq-server &amp;
</code></pre></div></div>

<p><b>Step 3:</b> Create an exchange in the rabbitmq. Go to <code class="highlighter-rouge">http://localhost:15672</code> (In my example, I‚Äôm binding a queue to the exchange. You can directly use a queue, but make sure to make corresponding changes in the code)</p>

<p><b>Step 4:</b> Clone the repo from <a target="_blank" href="https://github.com/rootcss/flink-rabbitmq.git ">here</a>: (will be explaining the codes inline)</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/rootcss/flink-rabbitmq.git
</code></pre></div></div>

<p><b>Step 5:</b> It‚Äôs built with maven. (Java) So, build it using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mvn clean package
</code></pre></div></div>

<p><b>Step 6:</b> Once built, You‚Äôre all set to run it now:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>flink run <span class="nt">-c</span> com.rootcss.flink.RabbitmqStreamProcessor target/flink-rabbitmq-0.1.jar
</code></pre></div></div>

<p><b>Step 7:</b> Check the logs at:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">tail</span> <span class="nt">-f</span> <span class="nv">$FLINK_HOME</span>/log/<span class="k">*</span>
</code></pre></div></div>

<p>and Flink‚Äôs dashboard at:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>http://localhost:8081/
</code></pre></div></div>

<p><b>Step 8:</b> Now, you can start publishing events from the RabbitMQ‚Äôs exchange and see the output in the logs.</p>

<p>Note that, I am not using any <b>Flink‚Äôs Sink</b> here (writing into the logs). You can use a file system like HDFS or a Database or even Rabbitmq (on a different channel ;))</p>

<h3 id="code-explanation">Code Explanation</h3>
<p>(This version might be a little different from the code in my repo. Just to keep this concise)</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Extend the RMQSource class, since we need to override a method to bind our queue</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">RabbitmqStreamProcessor</span> <span class="kd">extends</span> <span class="n">RMQSource</span><span class="o">{</span>

    <span class="c1">// This is mainly because we have to bind our queue to an exchange. If you are using a queue directly, you may skip it</span>
    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">setupQueue</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
        <span class="n">AMQP</span><span class="o">.</span><span class="na">Queue</span><span class="o">.</span><span class="na">DeclareOk</span> <span class="n">result</span> <span class="o">=</span> <span class="n">channel</span><span class="o">.</span><span class="na">queueDeclare</span><span class="o">(</span><span class="s">"simple_dev"</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>
        <span class="n">channel</span><span class="o">.</span><span class="na">queueBind</span><span class="o">(</span><span class="n">result</span><span class="o">.</span><span class="na">getQueue</span><span class="o">(),</span> <span class="s">"simple_exchange"</span><span class="o">,</span> <span class="s">"*"</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
        <span class="c1">// Setting up rabbitmq's configurations; ignore the default values</span>
        <span class="n">RMQConnectionConfig</span> <span class="n">connectionConfig</span> <span class="o">=</span> <span class="k">new</span> <span class="n">RMQConnectionConfig</span><span class="o">.</span><span class="na">Builder</span><span class="o">()</span>
                <span class="o">.</span><span class="na">setHost</span><span class="o">(</span><span class="s">"localhost"</span><span class="o">).</span><span class="na">setPort</span><span class="o">(</span><span class="mi">5672</span><span class="o">).</span><span class="na">setUserName</span><span class="o">(</span><span class="s">"rootcss"</span><span class="o">)</span>
                <span class="o">.</span><span class="na">setPassword</span><span class="o">(</span><span class="s">"password"</span><span class="o">).</span><span class="na">setVirtualHost</span><span class="o">(</span><span class="s">"/"</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>

        <span class="c1">// below ones are pretty intuitive class names, right?</span>
        <span class="n">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

        <span class="c1">// Finally adding Rabbitmq as source of the stream for Flink</span>
        <span class="n">DataStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">dataStream</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="k">new</span> <span class="n">RMQSource</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;(</span><span class="n">connectionConfig</span><span class="o">,</span>
                <span class="s">"simple_dev"</span><span class="o">,</span>
                <span class="k">new</span> <span class="nf">SimpleStringSchema</span><span class="o">()));</span>

        <span class="c1">// Accepting the events, and doing a flatMap to calculate string length of each event (to keep the things easy)</span>
        <span class="n">DataStream</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">dataStream</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">TextLengthCalculator</span><span class="o">());</span>

        <span class="c1">// action on the pairs, you can plug your Flink's Sink here as well.</span>
        <span class="n">pairs</span><span class="o">.</span><span class="na">print</span><span class="o">();</span>

        <span class="c1">// Start the execution of the worker</span>
        <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">();</span>
    <span class="o">}</span>
</code></pre></div></div>

<style>
pre code{
  white-space: pre;
}
</style>

<p>And, here is the beautiful web interface of Apache Flink:</p>

<p><img class="img-responsive" src="http://localhost:3000/assets/images/2016-11-12-apache-flink-rabbimq-streams-processor_1.png" alt="Flink Web Dashboard" /></p>
<p><br />
In the next post, I will be explaining how I bomarded events on both Spark &amp; Flink, to compare their endurance. Just for fun :-D</p>

<p>Stay Tuned!</p>
 ]]></content>
</entry>



</feed>
