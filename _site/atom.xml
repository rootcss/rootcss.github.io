<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
    <title type="text" xml:lang="en">Shekhar Singh | Big Data/Data Engineer | Full Stack Developer | Researcher | Data Science | DevOps | rootcss</title>
    <link type="application/atom+xml" rel="self" href="http://localhost:3000/atom.xml"/>
  
  <link href="http://localhost:3000/"/>
  <id>http://localhost:3000/</id>
  <updated>2017-02-02T20:48:34Z</updated>
  <author>
    <name>Shekhar Singh</name>
    <email>shekhar.singh@msn.com</email>
  </author>
  <rights type="text">Copyright © 2017 Shekhar Singh. All rights reserved.</rights>
  
  <entry>
  <title type="text">Spark Shell for Processing &amp;amp; Querying data in Cassandra</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/processing-cassandra-data-with-apache-spark-part-2.html" />
  <id>http://localhost:3000/processing-cassandra-data-with-apache-spark-part-2</id>
  <published>2017-01-24T00:00:00Z</published>
  <updated>2017-01-24T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>My previous <a href="processing-cassandra-data-with-apache-spark.html">post</a>  explains, how can you write a Spark job and execute it. In this post, I am writing down steps to initiate a Spark shell (pyspark or spark-shell), with a pre-established connection to Cassandra. In addition to this, I’ll write down some sample codes and their outputs, in order to show the usage of Spark Transformations/Actions.</p>

<p>To start the shell, just run this command on your shell.
<br /></p>
<pre>
pyspark \
      --packages com.datastax.spark:spark-cassandra-connector_2.10:1.5.0-M2 \
      --conf spark.cassandra.connection.host=192.168.56.101 \
      --conf spark.cassandra.auth.username=cassandra \
      --conf spark.cassandra.auth.password=cassandra
</pre>

<p>I think, most of the parameters are pretty intuitive. In short, we are just providing the dependency packages and cassandra connection configurations. Make sure you provide path to <code class="highlighter-rouge">pyspark</code> or add it in your <code class="highlighter-rouge">$PATH</code>.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>Python 2.7.11 <span class="o">(</span>default, Nov 10 2016, 03:37:47<span class="o">)</span>
<span class="o">[</span>GCC 4.2.1 Compatible Apple LLVM 8.0.0 <span class="o">(</span>clang-800.0.42.1<span class="o">)]</span> on darwin

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _<span class="se">\ \/</span> _ <span class="se">\/</span> _ <span class="sb">`</span>/ __/  <span class="s1">'_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Python version 2.7.11 (default, Nov 10 2016 03:37:47)
SparkContext available as sc, HiveContext available as sqlContext.


&gt;&gt;&gt; sqlContext.sql("""CREATE TEMPORARY TABLE roles \
...                   USING org.apache.spark.sql.cassandra \
...                   OPTIONS ( table "roles", \
...                             keyspace "system_auth", \
...                             cluster "rootCSSCluster", \
...                             pushdown "true") \
...               """)
DataFrame[]


&gt;&gt;&gt; sqlContext.sql('</span>SELECT <span class="k">*</span> from roles<span class="s1">').show()
+---------+---------+------------+---------+--------------------+
|     role|can_login|is_superuser|member_of|         salted_hash|
+---------+---------+------------+---------+--------------------+
|cassandra|     true|        true|       []|$2a$10$pQW3iGSC.m...|
+---------+---------+------------+---------+--------------------+


&gt;&gt;&gt; sqlContext.sql("""CREATE TEMPORARY TABLE compaction_history \
...                   USING org.apache.spark.sql.cassandra \
...                   OPTIONS ( table "compaction_history", \
...                             keyspace "system", \
...                             cluster "rootCSSCluster", \
...                             pushdown "true") \
...               """)
DataFrame[]


&gt;&gt;&gt; sqlContext.sql('</span>SELECT <span class="k">*</span> FROM compaction_history LIMIT 3<span class="s1">').show()
+--------------------+--------+---------+-----------------+--------------------+-------------+
|                  id|bytes_in|bytes_out|columnfamily_name|        compacted_at|keyspace_name|
+--------------------+--------+---------+-----------------+--------------------+-------------+
|b54ccf00-e236-11e...|   20729|     4301|   size_estimates|2017-01-24 18:42:...|       system|
|170ddba0-e23f-11e...|   12481|     3085| sstable_activity|2017-01-24 19:42:...|       system|
|914d2e70-e195-11e...|      41|        0|  schema_triggers|2017-01-23 23:28:...|       system|
+--------------------+--------+---------+-----------------+--------------------+-------------+


&gt;&gt;&gt; sqlContext.sql('</span>SELECT columnfamily_name, COUNT<span class="o">(</span><span class="k">*</span><span class="o">)</span> AS count FROM compaction_history GROUP BY columnfamily_name<span class="s1">').show()
+-----------------+-----+
|columnfamily_name|count|
+-----------------+-----+
| schema_functions|    2|
|schema_aggregates|    2|
|  schema_triggers|    2|
| schema_usertypes|    2|
|   size_estimates|    2|
| sstable_activity|    4|
+-----------------+-----+


&gt;&gt;&gt; sqlContext.sql('</span>SELECT SUM<span class="o">(</span>bytes_in<span class="o">)</span> AS sum FROM compaction_history<span class="s1">').show()
+-----+
|  sum|
+-----+
|73610|
+-----+


&gt;&gt;&gt; df_payload = sqlContext.sql('</span>SELECT bytes_in, bytes_out, columnfamily_name FROM compaction_history<span class="s1">')
&gt;&gt;&gt; df_payload.count()
14


# Simple Map Reduce example
&gt;&gt;&gt; df_payload\
...      .map(lambda x: (x['</span>columnfamily_name<span class="s1">'], 1))\
...      .reduceByKey(lambda x, y: x+y)\
...      .toDF()\
...      .show()
+-----------------+---+
|               _1| _2|
+-----------------+---+
|   size_estimates|  2|
| sstable_activity|  4|
| schema_usertypes|  2|
|schema_aggregates|  2|
| schema_functions|  2|
|  schema_triggers|  2|
+-----------------+---+

&gt;&gt;&gt;

</span></code></pre>
</div>

<p><br />
I’ll be adding more examples with time. :)</p>

<style>
pre code{
  white-space: pre;
}
</style>

 ]]></content>
</entry>


  <entry>
  <title type="text">Processing &amp;amp; Querying data in Cassandra with Apache Spark</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/processing-cassandra-data-with-apache-spark.html" />
  <id>http://localhost:3000/processing-cassandra-data-with-apache-spark</id>
  <published>2017-01-23T00:00:00Z</published>
  <updated>2017-01-23T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>This post is one of my <a href="new.html">Notes to Self</a> one. I’m simply going to write, how can you connect to Cassandra from Spark, run “SQL” queries and perform analysis on Cassandra’s data.</p>

<p><br />
Let’s get started.</p>

<p><br />
(Platform: Spark v1.6.0, Cassandra v2.7, macOS 10.12.1, Scala 2.11.7)</p>

<p>I’m going to use the package <code class="highlighter-rouge">spark-cassandra-connector</code> written by awesome <a href="http://www.datastax.com/">Datastax</a> guys.</p>

<p>Assuming you have already configured Cassandra &amp; Spark, it’s time to start writing a small Spark job.</p>

<p><b>Code with explanation</b>:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># imports necessary methods</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">SparkConf</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SQLContext</span>


<span class="c"># setup spark configuration object</span>
<span class="c"># this contains your cassandra connection parameters</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span>\
    <span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="s">"PySpark Cassandra"</span><span class="p">)</span> \
    <span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.cassandra.connection.host"</span><span class="p">,</span> <span class="s">"192.168.56.101"</span><span class="p">)</span>\
    <span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.cassandra.auth.username"</span><span class="p">,</span> <span class="s">"cassandra"</span><span class="p">)</span>\
    <span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.cassandra.auth.password"</span><span class="p">,</span> <span class="s">"cassandra"</span><span class="p">)</span>


<span class="c"># creates Spark Context with your cassandra configurations</span>
<span class="c"># local[*] represents that spark is going to use all cores of CPU for this job</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="s">"local[*]"</span><span class="p">,</span> <span class="s">"PySpark Cassandra"</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>


<span class="c"># creates Spark's sqlContext. This is going to be super useful.</span>
<span class="n">sqlContext</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>


<span class="c"># creates mapping with tables inside Cassandra for Spark</span>
<span class="c"># I am going to use "system_auth.roles" table here</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"""CREATE TEMPORARY TABLE roles </span><span class="se">\
</span><span class="s">                  USING org.apache.spark.sql.cassandra </span><span class="se">\
</span><span class="s">                  OPTIONS ( table "roles", </span><span class="se">\
</span><span class="s">                            keyspace "system_auth", </span><span class="se">\
</span><span class="s">                            cluster "rootCSSCluster", </span><span class="se">\
</span><span class="s">                            pushdown "true") </span><span class="se">\
</span><span class="s">              """</span><span class="p">)</span>

<span class="c"># you can create multiple mappings/temporary tables and write queries on it.</span>
<span class="c"># this in another table: "system.compaction_history"</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"""CREATE TEMPORARY TABLE compaction_history </span><span class="se">\
</span><span class="s">                  USING org.apache.spark.sql.cassandra </span><span class="se">\
</span><span class="s">                  OPTIONS ( table "compaction_history", </span><span class="se">\
</span><span class="s">                            keyspace "system", </span><span class="se">\
</span><span class="s">                            cluster "rootCSSCluster", </span><span class="se">\
</span><span class="s">                            pushdown "true") </span><span class="se">\
</span><span class="s">              """</span><span class="p">)</span>


<span class="c"># here's the query we are going to run</span>
<span class="n">query</span> <span class="o">=</span> <span class="s">"SELECT * FROM roles"</span>

<span class="k">print</span> <span class="s">"[Spark] Executing query: </span><span class="si">%</span><span class="s">s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="c"># The result of the query returns a dataframe</span>
<span class="n">df_payload</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="c"># priting the content of dataframe</span>
<span class="n">df_payload</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p><br /></p>

<p><b>Spark job Execution</b>:</p>

<p>To run your spark job, use the command below:</p>
<pre>
spark-submit --packages com.datastax.spark:spark-cassandra-connector_2.10:1.5.0-M2 myfile.py
</pre>

<p>(Note: Check <code class="highlighter-rouge">localhost:4040</code> in your browser for Spark UI)</p>

<p>--packages : This parameter tells Spark to download the external dependencies for the job.</p>

<p>In our case, we are using <code class="highlighter-rouge">spark-cassandra-connector</code>:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>groupId: com.datastax.spark
artifactId: spark-cassandra-connector_2.10
version: 1.5.0-M2
</code></pre>
</div>

<p><br /></p>

<p><b>Output</b>:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>&lt;all your logs will be printed here. including ivy logs&gt;
:
:
<span class="o">[</span>Spark] Executing query: <span class="k">select</span> <span class="k">*</span> from roles
+---------+---------+------------+---------+--------------------+
|     role|can_login|is_superuser|member_of|         salted_hash|
+---------+---------+------------+---------+--------------------+
|cassandra|     <span class="nb">true</span>|        <span class="nb">true</span>|       <span class="o">[]</span>|<span class="nv">$2a$10$pQW3iGSC</span>.m...|
+---------+---------+------------+---------+--------------------+
</code></pre>
</div>

<p><br />
Here, <code class="highlighter-rouge">df_payload</code> is DataFrame object. You can use all Spark’s <i>Transformations</i> &amp; <i>Actions</i> on this. (Check <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">here</a> for more details)</p>

<p><br />
<b>Second Part</b>: <u>Starting a Spark shell with Cassandra connection</u>
<br />
Steps for this is part of separate <a href="processing-cassandra-data-with-apache-spark-part-2.html">post</a>.</p>

<hr />

<p><b>Useful Links </b>:-</p>

<ol>
  <li>
    <p>I really want to thank guys at <a href="http://www.datastax.com/">Datastax</a>. They have written and open sourced, so many packages and drivers for Cassandra.</p>
  </li>
  <li>
    <p>You can contribute to <code class="highlighter-rouge">spark-cassandra-connector</code> <a href="https://github.com/datastax/spark-cassandra-connector">here</a>.</p>
  </li>
  <li>
    <p><a href="https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10/1.5.0-M2">Link</a> to <code class="highlighter-rouge">spark-cassandra-connector</code> maven repository.</p>
  </li>
</ol>
 ]]></content>
</entry>


  <entry>
  <title type="text">Processing Rabbitmq's Stream with &quot;Apache Flink&quot;</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/apache-flink-rabbimq-streams-processor.html" />
  <id>http://localhost:3000/apache-flink-rabbimq-streams-processor</id>
  <published>2016-11-12T00:00:00Z</published>
  <updated>2016-11-12T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>I love <a target="_blank" href="http://spark.apache.org/">Apache Spark</a>. Not just becacuse of it’s capability to adapt to so many use-cases, but because it’s one of shining star in the <i>Distributing Computing</i> world, has a great design and superb community backing.</p>

<p>However, one of the features I’d want enhancement in is, the way it processes the streams. Spark processes the streams in a <b>micro-batch</b> manner i.e, you set a time interval (could be any value), and Spark will process the events collected in that interval, in batch. This is where Apache Flink comes in!</p>

<p><a target="_blank" href="http://spark.apache.org/">Apache Flink</a> is often comapred with Spark. I feel Spark is far ahead of Flink, not just in technology; but even community backing of Spark is very big, compared to Flink.</p>

<p>Anyways, this post is not about comparing them, but to provide a detailed example of processing a RabbitMQ’s stream using Apache Flink.</p>

<p><b>Step 1:</b> Install Rabbitmq, Apache Flink in your system. Both installations are very straightforward.</p>

<p><b>Step 2:</b> Start Rabbitmq server</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>rabbitmq-server &amp;
</code></pre>
</div>

<p><b>Step 3:</b> Create an exchange in the rabbitmq. Go to <code class="highlighter-rouge">http://localhost:15672</code> (In my example, I’m binding a queue to the exchange. You can directly use a queue, but make sure to make corresponding changes in the code)</p>

<p><b>Step 4:</b> Clone the repo from <a target="_blank" href="https://github.com/rootcss/flink-rabbitmq.git ">here</a>: (will be explaining the codes inline)</p>

<div class="language-html highlighter-rouge"><pre class="highlight"><code>git clone https://github.com/rootcss/flink-rabbitmq.git
</code></pre>
</div>

<p><b>Step 5:</b> It’s built with maven. (Java) So, build it using:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>mvn clean package
</code></pre>
</div>

<p><b>Step 6:</b> Once built, You’re all set to run it now:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>flink run -c com.rootcss.flink.RabbitmqStreamProcessor target/flink-rabbitmq-0.1.jar
</code></pre>
</div>

<p><b>Step 7:</b> Check the logs at:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>tail -f <span class="nv">$FLINK_HOME</span>/log/<span class="k">*</span>
</code></pre>
</div>

<p>and Flink’s dashboard at:</p>

<div class="language-html highlighter-rouge"><pre class="highlight"><code>http://localhost:8081/
</code></pre>
</div>

<p><b>Step 8:</b> Now, you can start publishing events from the RabbitMQ’s exchange and see the output in the logs.</p>

<p>Note that, I am not using any <b>Flink’s Sink</b> here (writing into the logs). You can use a file system like HDFS or a Database or even Rabbitmq (on a different channel ;))</p>

<h3 id="code-explanation">Code Explanation</h3>
<p>(This version might be a little different from the code in my repo. Just to keep this concise)</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="c1">// Extend the RMQSource class, since we need to override a method to bind our queue</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">RabbitmqStreamProcessor</span> <span class="kd">extends</span> <span class="n">RMQSource</span><span class="o">{</span>

    <span class="c1">// This is mainly because we have to bind our queue to an exchange. If you are using a queue directly, you may skip it</span>
    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">setupQueue</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
        <span class="n">AMQP</span><span class="o">.</span><span class="na">Queue</span><span class="o">.</span><span class="na">DeclareOk</span> <span class="n">result</span> <span class="o">=</span> <span class="n">channel</span><span class="o">.</span><span class="na">queueDeclare</span><span class="o">(</span><span class="s">"simple_dev"</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>
        <span class="n">channel</span><span class="o">.</span><span class="na">queueBind</span><span class="o">(</span><span class="n">result</span><span class="o">.</span><span class="na">getQueue</span><span class="o">(),</span> <span class="s">"simple_exchange"</span><span class="o">,</span> <span class="s">"*"</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
        <span class="c1">// Setting up rabbitmq's configurations; ignore the default values</span>
        <span class="n">RMQConnectionConfig</span> <span class="n">connectionConfig</span> <span class="o">=</span> <span class="k">new</span> <span class="n">RMQConnectionConfig</span><span class="o">.</span><span class="na">Builder</span><span class="o">()</span>
                <span class="o">.</span><span class="na">setHost</span><span class="o">(</span><span class="s">"localhost"</span><span class="o">).</span><span class="na">setPort</span><span class="o">(</span><span class="mi">5672</span><span class="o">).</span><span class="na">setUserName</span><span class="o">(</span><span class="s">"rootcss"</span><span class="o">)</span>
                <span class="o">.</span><span class="na">setPassword</span><span class="o">(</span><span class="s">"password"</span><span class="o">).</span><span class="na">setVirtualHost</span><span class="o">(</span><span class="s">"/"</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>

        <span class="c1">// below ones are pretty intuitive class names, right?</span>
        <span class="n">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

        <span class="c1">// Finally adding Rabbitmq as source of the stream for Flink</span>
        <span class="n">DataStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">dataStream</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="k">new</span> <span class="n">RMQSource</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;(</span><span class="n">connectionConfig</span><span class="o">,</span>
                <span class="s">"simple_dev"</span><span class="o">,</span>
                <span class="k">new</span> <span class="nf">SimpleStringSchema</span><span class="o">()));</span>

        <span class="c1">// Accepting the events, and doing a flatMap to calculate string length of each event (to keep the things easy)</span>
        <span class="n">DataStream</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">dataStream</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">TextLengthCalculator</span><span class="o">());</span>

        <span class="c1">// action on the pairs, you can plug your Flink's Sink here as well.</span>
        <span class="n">pairs</span><span class="o">.</span><span class="na">print</span><span class="o">();</span>

        <span class="c1">// Start the execution of the worker</span>
        <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">();</span>
    <span class="o">}</span>
</code></pre>
</div>

<style>
pre code{
  white-space: pre;
}
</style>

<p>And, here is the beautiful web interface of Apache Flink:</p>

<p><img class="img-responsive" src="assets/images/2016-11-12-apache-flink-rabbimq-streams-processor_1.png" alt="Flink Web Dashboard" /></p>
<p><br />
In the next post, I will be explaining how I bomarded events on both Spark &amp; Flink, to compare their endurance. Just for fun :-D</p>

<p>Stay Tuned!</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">Writing Apache Spark workers with &quot;Simple Spark Lib&quot;</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/apache-spark-worker-with-simple-spark-lib.html" />
  <id>http://localhost:3000/apache-spark-worker-with-simple-spark-lib</id>
  <published>2016-11-11T00:00:00Z</published>
  <updated>2016-11-11T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p><a target="_blank" href="http://spark.apache.org/">Apache Spark</a> is a great project, could be plugged with most of the data sources/databases eg, HDFS, Cassandra, MongoDB, Kafka, Postgres, Redshift etc. I have been using Spark for ad-hoc querying, bunch of Aggregations &amp; Segregations over Cassandra from a long time and noticed that, every time I used to write (or paste) same code for configuration &amp; connection. Also, I knew when someone else wants to do the similar work from my team, he/she will have to do the same thing, including learning what that means and understanding it. Think of someone doing that, if he is using Spark for the first time?</p>

<p><br />TLDR;</p>

<p>I decided to write a wrapper over <code class="highlighter-rouge">PySpark</code> which obviously supports Cassandra, Redshift etc. It primarily provided following two advantages:</p>

<ol>
  <li>I never repeated myself while writing the workers again</li>
  <li>My Team members do not need to figure out those Spark specific code in order to do some simple ad-hoc tasks</li>
</ol>

<p>I named it “<code class="highlighter-rouge">Simple Spark Lib</code>” and, here’s how to use it:</p>

<p>Step 1: Clone the repo from <a href="https://github.com/rootcss/simple_spark_lib">here</a>:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">git clone https://github.com/rootcss/simple_spark_lib.git</code></pre></figure>

<p>Step 2: Install the library:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">python setup.py install</code></pre></figure>

<p>Step 3: Write the worker:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># First, import the library</span>
<span class="kn">from</span> <span class="nn">simple_spark_lib</span> <span class="kn">import</span> <span class="n">SimpleSparkCassandraWorkflow</span>

<span class="c"># Define connection configuration for cassandra</span>
<span class="n">cassandra_connection_config</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'host'</span><span class="p">:</span>     <span class="s">'192.168.56.101'</span><span class="p">,</span>
  <span class="s">'username'</span><span class="p">:</span> <span class="s">'cassandra'</span><span class="p">,</span>
  <span class="s">'password'</span><span class="p">:</span> <span class="s">'cassandra'</span>
<span class="p">}</span>

<span class="c"># Define Cassandra Schema information</span>
<span class="n">cassandra_config</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'cluster'</span><span class="p">:</span> <span class="s">'rootCSSCluster'</span><span class="p">,</span>
  <span class="s">'tables'</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">'api_events'</span><span class="p">:</span> <span class="s">'events_production.api_events'</span><span class="p">,</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="c"># Initiate your workflow</span>
<span class="n">workflow</span> <span class="o">=</span> <span class="n">SimpleSparkCassandraWorkflow</span><span class="p">(</span><span class="n">appName</span><span class="o">=</span><span class="s">"Simple Example Worker"</span><span class="p">)</span>

<span class="c"># Setup the workflow with configurations</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">cassandra_connection_config</span><span class="p">,</span> <span class="n">cassandra_config</span><span class="p">)</span>

<span class="c"># Run your favourite query</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">workflow</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="s">"SELECT * FROM api_events LIMIT 10"</span><span class="p">)</span>

<span class="k">print</span> <span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p>Step 4: Save it &amp; Execute the worker:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">simple-runner my_spark_woker.py -d cassandra</code></pre></figure>

<p><code class="highlighter-rouge">simple_spark_lib</code> enables you to use the capability of spark without writing the actual Spark codes. I made it public, hoping it might be useful to someone else too.</p>

<p>If you are interested, go through other examples in the repo and feel free to contribute. :-)</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">RabbitMQ - Automated deletion of 1000s of queues</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/rabbitmq-queues-overflow-automated-deletion.html" />
  <id>http://localhost:3000/rabbitmq-queues-overflow-automated-deletion</id>
  <published>2016-03-24T00:00:00Z</published>
  <updated>2016-03-24T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>Recently, I was using <a href="https://github.com/jondot/sneakers">sneakers</a> for rails, which is a small framework for Ruby and RabbitMQ. One issue with sneakers is that, if you have faulty configuration for a queue or you do not provide a queue name, it leaves it upto rabbitmq to define it. So, for some reason (which I don’t want to focus on), we had more than 1600 queues created on that particular exchange, and unfortunately they were not Auto-delete and we didn’t want other exchanges and queues to get hurt because of this ;)</p>

<p>Anyway, Now the challenge was to delete them, because for that you have to manually click on each queue, select ‘Delete’ on the new page and finally, confirm it on pop-up.</p>

<p>I was like, :O</p>

<p>Anyways, Thanks to handy APIs of rabbitmq, these are the following few commands I used in order to delete them quickly. (rabbitmq generally creates default queues with names like <code class="highlighter-rouge">amq.gen--*</code>)</p>

<p>First let’s list all the queues in the form of bash arrays:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">rabbitmqadmin --host<span class="o">=</span>&lt;mqserver.hostname.com&gt; --port<span class="o">=</span>443 --ssl --vhost<span class="o">=</span>&lt;your_vhost&gt; --username<span class="o">=</span>&lt;your_username&gt; --password<span class="o">=</span>&lt;your_password&gt; list queues | awk <span class="s1">'{print $2}'</span> | grep amq.gen  | xargs | sed -e <span class="s1">'s/ /" "/g'</span></code></pre></figure>

<p>Now copy the output of it, declare as an array and run a loop to delete them all.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">declare</span> -a <span class="nv">arr</span><span class="o">=(</span><span class="s2">"amq.gen--PxKpFBHkIxxebJEwbmV6g"</span> <span class="s2">"amq.gen--Q6BeLdfGHsXY6RgVmu8Ig"</span> <span class="s2">"amq.gen--WI0hRAHCOkPIrEULYc1vQ"</span> <span class="s2">"amq.gen--XufS0RrnfZUXyf0Rt1tAg"</span> <span class="s2">"amq.gen--_NXdwlSHYDJwGDiuX8_XA"</span> ......<span class="o">)</span>

<span class="k">for </span>i <span class="k">in</span> <span class="s2">"</span><span class="k">${</span><span class="nv">arr</span><span class="p">[@]</span><span class="k">}</span><span class="s2">"</span>
<span class="k">do
   </span><span class="nb">echo</span> <span class="s2">"</span><span class="nv">$i</span><span class="s2">"</span>
   rabbitmqadmin --host<span class="o">=</span>&lt;mqserver.hostname.com&gt; --port<span class="o">=</span>443 --ssl --vhost<span class="o">=</span>&lt;your_vhost&gt; --username<span class="o">=</span>&lt;your_username&gt; --password<span class="o">=</span>&lt;your_password&gt; delete queue <span class="nv">name</span><span class="o">=</span><span class="s2">"</span><span class="nv">$i</span><span class="s2">"</span>
<span class="k">done</span></code></pre></figure>

<p>That’s it. These small hacks makes me fall in love with programming everyday &lt;3</p>

<p>Thanks!</p>

<style type="text/css">
pre {
    white-space: pre-wrap;
}
</style>

 ]]></content>
</entry>


  <entry>
  <title type="text">Linux I/O redirection examples</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/linux-io-redirections.html" />
  <id>http://localhost:3000/linux-io-redirections</id>
  <published>2016-02-15T00:00:00Z</published>
  <updated>2016-02-15T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>I/O redirections are one of the prettiest things we have in linux (IMO!) Following are commands and their usage.
<br /><br />
<code class="highlighter-rouge">command_output &gt;&gt; file </code>
  Redirects stdout to a file. Creates the file if not present, otherwise appends.</p>

<p><code class="highlighter-rouge"> &gt; filename </code>
  Truncates the file to zero length. If file is not present, creates zero-length file (same effect as <code class="highlighter-rouge">touch</code>).</p>

<p><code class="highlighter-rouge"> 1&gt;filename </code>
  Redirects stdout to the file “filename”.</p>

<p><code class="highlighter-rouge"> 1&gt;&gt;filename </code>
  Redirects and appends stdout to file “filename”.</p>

<p><code class="highlighter-rouge"> 2&gt;filename </code>
  Redirects stderr to file “filename”.</p>

<p><code class="highlighter-rouge"> 2&gt;&gt;filename </code>
  Redirects and appends stderr to file “filename”.</p>

<p><code class="highlighter-rouge"> &amp;&gt;filename </code>
  Redirects both stdout and stderr to file “filename”.</p>

<p><code class="highlighter-rouge"> 2&gt;&amp;1 </code>
  Redirects stderr to stdout. Error messages get sent to same place as standard output.</p>
<hr />

<p>Some quality explanation now ;) Take the example of this command:
<br /><code class="highlighter-rouge"> cmd &gt;&gt; file.log 2&gt;&amp;1 </code>
<br />
This command will redirect all the output of command(cmd) into <code class="highlighter-rouge">file.log</code>.<br />
<code class="highlighter-rouge">2</code> refers to Second file descriptor of the process i.e., stderr<br />
<code class="highlighter-rouge">&gt;</code> refers to redirection<br />
<code class="highlighter-rouge">&amp;1</code> means that the target of redirection would be same as <code class="highlighter-rouge">1</code> i.e, first descriptor i.e, stdout.<br /></p>

<p>Thanks!</p>

<style type="text/css">
code {
    font-weight: bold;
    font-size: 18px;
    background: #ddd;
    padding: 3px;
}   
</style>

 ]]></content>
</entry>


  <entry>
  <title type="text">PostgreSQL - update timestamp when row(s) is updated</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/postgres-update-timestamp-when-row-is-updated.html" />
  <id>http://localhost:3000/postgres-update-timestamp-when-row-is-updated</id>
  <published>2015-11-24T00:00:00Z</published>
  <updated>2015-11-24T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>In PostgreSQL, if you want to set current timestamp as default value, you can simply keep a column’s default expression as <code class="highlighter-rouge">now()</code>. However, by default there is no function defined to update the timestamp when a particular row (or multiple rows) need to be updated.</p>

<p>In such scenario, you may create your custom method and trigger it using <b>PostgreSQL’s Triggers</b>. Following snippet will make it more clear:</p>

<p>Here, we are creating a new method, <code class="highlighter-rouge">method_get_updated_at()</code></p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">OR</span> <span class="k">REPLACE</span> <span class="k">FUNCTION</span> <span class="n">method_get_updated_at</span><span class="p">()</span> <span class="k">RETURNS</span> <span class="k">TRIGGER</span>
<span class="k">LANGUAGE</span> <span class="n">plpgsql</span>
<span class="k">AS</span> <span class="err">$$</span>
    <span class="k">BEGIN</span>
      <span class="k">NEW</span><span class="p">.</span><span class="o">&lt;</span><span class="k">column_name</span><span class="o">&gt;</span> <span class="o">=</span> <span class="n">now</span><span class="p">();</span>
      <span class="k">RETURN</span> <span class="k">NEW</span><span class="p">;</span>
    <span class="k">END</span><span class="p">;</span>
<span class="err">$$</span><span class="p">;</span></code></pre></figure>

<p>Once it is created, use the following snippet to trigger it:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TRIGGER</span> <span class="n">trigger_</span><span class="o">&lt;</span><span class="k">column_name</span><span class="o">&gt;</span>
<span class="k">BEFORE</span> <span class="k">UPDATE</span> <span class="k">ON</span> <span class="o">&lt;</span><span class="k">table_name</span><span class="o">&gt;</span>
<span class="k">FOR</span> <span class="k">EACH</span> <span class="k">ROW</span>
<span class="k">EXECUTE</span> <span class="k">PROCEDURE</span> <span class="n">method_get_updated_at</span><span class="p">();</span></code></pre></figure>

<p>If you want to delete a Trigger, you can use this simple query:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">DROP</span> <span class="k">TRIGGER</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">trigger_</span><span class="o">&lt;</span><span class="k">column_name</span><span class="o">&gt;</span> <span class="k">ON</span> <span class="o">&lt;</span><span class="k">table_name</span><span class="o">&gt;</span></code></pre></figure>

<p><b>Note:</b> Please update the <table_name> and <column_name> accordingly and execute the code for your particular database. Also, note that, some web frameworks (like Rails) manage such columns(created_at, updated_at) automatically.</column_name></table_name></p>

<p>ALso, if you want to view all existing methods, run this query:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span>  <span class="n">p</span><span class="p">.</span><span class="n">proname</span>
<span class="k">FROM</span>    <span class="n">pg_catalog</span><span class="p">.</span><span class="n">pg_namespace</span> <span class="n">n</span>
<span class="k">JOIN</span>    <span class="n">pg_catalog</span><span class="p">.</span><span class="n">pg_proc</span> <span class="n">p</span>
<span class="k">ON</span>      <span class="n">p</span><span class="p">.</span><span class="n">pronamespace</span> <span class="o">=</span> <span class="n">n</span><span class="p">.</span><span class="n">oid</span>
<span class="k">WHERE</span>   <span class="n">n</span><span class="p">.</span><span class="n">nspname</span> <span class="o">=</span> <span class="s1">'public'</span></code></pre></figure>

<p>And, run this query to view all Triggers:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">pg_trigger</span><span class="p">;</span></code></pre></figure>

<p>Thanks!</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">Accessing PostgreSQL server through a SSH Tunnel</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/accessing-postgreSQL-through-ssh-tunnel.html" />
  <id>http://localhost:3000/accessing-postgreSQL-through-ssh-tunnel</id>
  <published>2015-11-20T00:00:00Z</published>
  <updated>2015-11-20T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p><b>Step 1:</b> Check the SSH connectivity with the server, verify username and password.</p>

<p><b>Step 2:</b> Create the tunnel in your local system by executing the following command (It will prompt for password):</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">ssh -fNg -L 5555:localhost:5432 &lt;user&gt;@&lt;server&gt;</code></pre></figure>

<p><b>Step 3:</b> Now, open your PostgreSQL client (eg, <code class="highlighter-rouge">pgAdmin 3</code> or <code class="highlighter-rouge">DBeaver</code> or <code class="highlighter-rouge">Postico</code> for OS X or <code class="highlighter-rouge">Terminal</code>) and fill in the connection details as usual. Check the image below.</p>

<p><img class="img-responsive" src="assets/images/postico-port-forwarding.png" alt="Postico DB connection" /></p>

<p><b>Note:</b> Yes, you’ll have to use <code class="highlighter-rouge">'localhost'</code>.</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">Welcome!</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/new.html" />
  <id>http://localhost:3000/new</id>
  <published>2011-12-31T00:00:00Z</published>
  <updated>2011-12-31T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>Hey! Welcome to my blog. This blog is going to be more like Notes to self. However, sometimes I’d post random ideas, thoughts, thinking-out-louds.</p>

<p>It’s mostly going to be technical, mostly.</p>
 ]]></content>
</entry>



</feed>
