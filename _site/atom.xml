<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
    <title type="text" xml:lang="en">Blog of Shekhar Singh a.k.a rootcss | Big Data/Data Engineer | Full Stack Developer | Researcher | Data Science | DevOps | Security | Shekhar Singh</title>
    <link type="application/atom+xml" rel="self" href="http://localhost:3000/atom.xml"/>
  
  <link href="http://localhost:3000/"/>
  <id>http://localhost:3000/</id>
  <updated>2016-11-15T10:21:10Z</updated>
  <author>
    <name>Shekhar Singh</name>
    <email>shekhar.singh@msn.com</email>
  </author>
  <rights type="text">Copyright © 2016 Shekhar Singh. All rights reserved.</rights>
  
  <entry>
  <title type="text">Processing Rabbitmq's Stream with &quot;Apache Flink&quot;</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/apache-flink-rabbimq-streams-processor.html" />
  <id>http://localhost:3000/apache-flink-rabbimq-streams-processor</id>
  <published>2016-11-12T00:00:00Z</published>
  <updated>2016-11-12T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>I love <a target="_blank" href="http://spark.apache.org/">Apache Spark</a>. Not just becacuse of it’s capability to adapt to so many use-cases, but because it’s one of shining star in the <i>Distributing Computing</i> world, has a great design and superb community backing.</p>

<p>However, one of the features I’d want enhancement in is, the way it processes the streams. Spark processes the streams in a <b>micro-batch</b> manner i.e, you set a time interval (could be any value), and Spark will process the events collected in that interval, in batch. This is where Apache Flink comes in!</p>

<p><a target="_blank" href="http://spark.apache.org/">Apache Flink</a> is often comapred with Spark. I feel Spark is far ahead of Flink, not just in technology; but even community backing of Spark is very big, compared to Flink.</p>

<p>Anyways, this post is not about comparing them, but to provide a detailed example of processing a RabbitMQ’s stream using Apache Flink.</p>

<p><b>Step 1:</b> Install Rabbitmq, Apache Flink in your system. Both installations are very straightforward.</p>

<p><b>Step 2:</b> Start Rabbitmq server</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>rabbitmq-server &amp;
</code></pre>
</div>

<p><b>Step 3:</b> Create an exchange in the rabbitmq. Go to <code class="highlighter-rouge">http://localhost:15672</code> (In my example, I’m binding a queue to the exchange. You can directly use a queue, but make sure to make corresponding changes in the code)</p>

<p><b>Step 4:</b> Clone the repo from <a target="_blank" href="https://github.com/rootcss/flink-rabbitmq.git ">here</a>: (will be explaining the codes inline)</p>

<div class="language-html highlighter-rouge"><pre class="highlight"><code>git clone https://github.com/rootcss/flink-rabbitmq.git
</code></pre>
</div>

<p><b>Step 5:</b> It’s built with maven. (Java) So, build it using:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>mvn clean package
</code></pre>
</div>

<p><b>Step 6:</b> Once built, You’re all set to run it now:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>flink run -c com.rootcss.flink.RabbitmqStreamProcessor target/flink-rabbitmq-0.1.jar
</code></pre>
</div>

<p><b>Step 7:</b> Check the logs at:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>tail -f <span class="nv">$FLINK_HOME</span>/log/<span class="k">*</span>
</code></pre>
</div>

<p>and Flink’s dashboard at:</p>

<div class="language-html highlighter-rouge"><pre class="highlight"><code>http://localhost:8081/
</code></pre>
</div>

<p><b>Step 8:</b> Now, you can start publishing events from the RabbitMQ’s exchange and see the output in the logs.</p>

<p>Note that, I am not using any <b>Flink’s Sink</b> here (writing into the logs). You can use a file system like HDFS or a Database or even Rabbitmq (on a different channel ;))</p>

<h3 id="code-explanation">Code Explanation</h3>
<p>(This version might be a little different from the code in my repo. Just to keep this concise)</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="c1">// Extend the RMQSource class, since we need to override a method to bind our queue</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">RabbitmqStreamProcessor</span> <span class="kd">extends</span> <span class="n">RMQSource</span><span class="o">{</span>

    <span class="c1">// This is mainly because we have to bind our queue to an exchange. If you are using a queue directly, you may skip it</span>
    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">setupQueue</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
        <span class="n">AMQP</span><span class="o">.</span><span class="na">Queue</span><span class="o">.</span><span class="na">DeclareOk</span> <span class="n">result</span> <span class="o">=</span> <span class="n">channel</span><span class="o">.</span><span class="na">queueDeclare</span><span class="o">(</span><span class="s">"simple_dev"</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>
        <span class="n">channel</span><span class="o">.</span><span class="na">queueBind</span><span class="o">(</span><span class="n">result</span><span class="o">.</span><span class="na">getQueue</span><span class="o">(),</span> <span class="s">"simple_exchange"</span><span class="o">,</span> <span class="s">"*"</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
        <span class="c1">// Setting up rabbitmq's configurations; ignore the default values</span>
        <span class="n">RMQConnectionConfig</span> <span class="n">connectionConfig</span> <span class="o">=</span> <span class="k">new</span> <span class="n">RMQConnectionConfig</span><span class="o">.</span><span class="na">Builder</span><span class="o">()</span>
                <span class="o">.</span><span class="na">setHost</span><span class="o">(</span><span class="s">"localhost"</span><span class="o">).</span><span class="na">setPort</span><span class="o">(</span><span class="mi">5672</span><span class="o">).</span><span class="na">setUserName</span><span class="o">(</span><span class="s">"rootcss"</span><span class="o">)</span>
                <span class="o">.</span><span class="na">setPassword</span><span class="o">(</span><span class="s">"password"</span><span class="o">).</span><span class="na">setVirtualHost</span><span class="o">(</span><span class="s">"/"</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>

        <span class="c1">// below ones are pretty intuitive class names, right?</span>
        <span class="n">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

        <span class="c1">// Finally adding Rabbitmq as source of the stream for Flink</span>
        <span class="n">DataStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">dataStream</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="k">new</span> <span class="n">RMQSource</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;(</span><span class="n">connectionConfig</span><span class="o">,</span>
                <span class="s">"simple_dev"</span><span class="o">,</span>
                <span class="k">new</span> <span class="nf">SimpleStringSchema</span><span class="o">()));</span>

        <span class="c1">// Accepting the events, and doing a flatMap to calculate string length of each event (to keep the things easy)</span>
        <span class="n">DataStream</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">dataStream</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">TextLengthCalculator</span><span class="o">());</span>

        <span class="c1">// action on the pairs, you can plug your Flink's Sink here as well.</span>
        <span class="n">pairs</span><span class="o">.</span><span class="na">print</span><span class="o">();</span>

        <span class="c1">// Start the execution of the worker</span>
        <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">();</span>
    <span class="o">}</span>
</code></pre>
</div>

<style>
pre code{
  white-space: pre;
}
</style>

<p>And, here is the beautiful web interface of Apache Flink:</p>

<p><img class="img-responsive" src="assets/images/2016-11-12-apache-flink-rabbimq-streams-processor_1.png" alt="Flink Web Dashboard" /></p>
<p><br />
In the next post, I will be explaining how I bomarded events on both Spark &amp; Flink, to compare their endurance. Just for fun :-D</p>

<p>Stay Tuned!</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">Writing Apache Spark workers with &quot;Simple Spark Lib&quot;</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/apache-spark-worker-with-simple-spark-lib.html" />
  <id>http://localhost:3000/apache-spark-worker-with-simple-spark-lib</id>
  <published>2016-11-11T00:00:00Z</published>
  <updated>2016-11-11T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p><a target="_blank" href="http://spark.apache.org/">Apache Spark</a> is a great project, could be plugged with most of the data sources/databases eg, HDFS, Cassandra, MongoDB, Kafka, Postgres, Redshift etc. I have been using Spark for ad-hoc querying, bunch of Aggregations &amp; Segregations over Cassandra from a long time and noticed that, every time I used to write (or paste) same code for configuration &amp; connection. Also, I knew when someone else wants to do the similar work from my team, he/she will have to do the same thing, including learning what that means and understanding it. Think of someone doing that, if he is using Spark for the first time?</p>

<p><br />TLDR;</p>

<p>I decided to write a wrapper over <code class="highlighter-rouge">PySpark</code> which obviously supports Cassandra, Redshift etc. It primarily provided following two advantages:</p>

<ol>
  <li>I never repeated myself while writing the workers again</li>
  <li>My Team members do not need to figure out those Spark specific code in order to do some simple ad-hoc tasks</li>
</ol>

<p>I named it “<code class="highlighter-rouge">Simple Spark Lib</code>” and, here’s how to use it:</p>

<p>Step 1: Clone the repo from <a href="https://github.com/rootcss/simple_spark_lib">here</a>:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">git clone https://github.com/rootcss/simple_spark_lib.git</code></pre></figure>

<p>Step 2: Install the library:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">python setup.py install</code></pre></figure>

<p>Step 3: Write the worker:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># First, import the library</span>
<span class="kn">from</span> <span class="nn">simple_spark_lib</span> <span class="kn">import</span> <span class="n">SimpleSparkCassandraWorkflow</span>

<span class="c"># Define connection configuration for cassandra</span>
<span class="n">cassandra_connection_config</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'host'</span><span class="p">:</span>     <span class="s">'192.168.56.101'</span><span class="p">,</span>
  <span class="s">'username'</span><span class="p">:</span> <span class="s">'cassandra'</span><span class="p">,</span>
  <span class="s">'password'</span><span class="p">:</span> <span class="s">'cassandra'</span>
<span class="p">}</span>

<span class="c"># Define Cassandra Schema information</span>
<span class="n">cassandra_config</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'cluster'</span><span class="p">:</span> <span class="s">'rootCSSCluster'</span><span class="p">,</span>
  <span class="s">'tables'</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">'api_events'</span><span class="p">:</span> <span class="s">'events_production.api_events'</span><span class="p">,</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="c"># Initiate your workflow</span>
<span class="n">workflow</span> <span class="o">=</span> <span class="n">SimpleSparkCassandraWorkflow</span><span class="p">(</span><span class="n">appName</span><span class="o">=</span><span class="s">"Simple Example Worker"</span><span class="p">)</span>

<span class="c"># Setup the workflow with configurations</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">cassandra_connection_config</span><span class="p">,</span> <span class="n">cassandra_config</span><span class="p">)</span>

<span class="c"># Run your favourite query</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">workflow</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="s">"SELECT * FROM api_events LIMIT 10"</span><span class="p">)</span>

<span class="k">print</span> <span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p>Step 4: Save it &amp; Execute the worker:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">simple-runner my_spark_woker.py -d cassandra</code></pre></figure>

<p><code class="highlighter-rouge">simple_spark_lib</code> enables you to use the capability of spark without writing the actual Spark codes. I made it public, hoping it might be useful to someone else too.</p>

<p>If you are interested, go through other examples in the repo and feel free to contribute. :-)</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">RabbitMQ - Automated deletion of 1000s of queues</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/rabbitmq-queues-overflow-automated-deletion.html" />
  <id>http://localhost:3000/rabbitmq-queues-overflow-automated-deletion</id>
  <published>2016-03-24T00:00:00Z</published>
  <updated>2016-03-24T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>Recently, I was using <a href="https://github.com/jondot/sneakers">sneakers</a> for rails, which is a small framework for Ruby and RabbitMQ. One issue with sneakers is that, if you have faulty configuration for a queue or you do not provide a queue name, it leaves it upto rabbitmq to define it. So, for some reason (which I don’t want to focus on), we had more than 1600 queues created on that particular exchange, and unfortunately they were not Auto-delete and we didn’t want other exchanges and queues to get hurt because of this ;)</p>

<p>Anyway, Now the challenge was to delete them, because for that you have to manually click on each queue, select ‘Delete’ on the new page and finally, confirm it on pop-up.</p>

<p>I was like, :O</p>

<p>Anyways, Thanks to handy APIs of rabbitmq, these are the following few commands I used in order to delete them quickly. (rabbitmq generally creates default queues with names like <code class="highlighter-rouge">amq.gen--*</code>)</p>

<p>First let’s list all the queues in the form of bash arrays:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">rabbitmqadmin --host<span class="o">=</span>&lt;mqserver.hostname.com&gt; --port<span class="o">=</span>443 --ssl --vhost<span class="o">=</span>&lt;your_vhost&gt; --username<span class="o">=</span>&lt;your_username&gt; --password<span class="o">=</span>&lt;your_password&gt; list queues | awk <span class="s1">'{print $2}'</span> | grep amq.gen  | xargs | sed -e <span class="s1">'s/ /" "/g'</span></code></pre></figure>

<p>Now copy the output of it, declare as an array and run a loop to delete them all.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">declare</span> -a <span class="nv">arr</span><span class="o">=(</span><span class="s2">"amq.gen--PxKpFBHkIxxebJEwbmV6g"</span> <span class="s2">"amq.gen--Q6BeLdfGHsXY6RgVmu8Ig"</span> <span class="s2">"amq.gen--WI0hRAHCOkPIrEULYc1vQ"</span> <span class="s2">"amq.gen--XufS0RrnfZUXyf0Rt1tAg"</span> <span class="s2">"amq.gen--_NXdwlSHYDJwGDiuX8_XA"</span> ......<span class="o">)</span>

<span class="k">for </span>i <span class="k">in</span> <span class="s2">"</span><span class="k">${</span><span class="nv">arr</span><span class="p">[@]</span><span class="k">}</span><span class="s2">"</span>
<span class="k">do
   </span><span class="nb">echo</span> <span class="s2">"</span><span class="nv">$i</span><span class="s2">"</span>
   rabbitmqadmin --host<span class="o">=</span>&lt;mqserver.hostname.com&gt; --port<span class="o">=</span>443 --ssl --vhost<span class="o">=</span>&lt;your_vhost&gt; --username<span class="o">=</span>&lt;your_username&gt; --password<span class="o">=</span>&lt;your_password&gt; delete queue <span class="nv">name</span><span class="o">=</span><span class="s2">"</span><span class="nv">$i</span><span class="s2">"</span>
<span class="k">done</span></code></pre></figure>

<p>That’s it. These small hacks makes me fall in love with programming everyday &lt;3</p>

<p>Thanks!</p>

<style type="text/css">
pre {
    white-space: pre-wrap;
}
</style>

 ]]></content>
</entry>


  <entry>
  <title type="text">Linux I/O redirection examples</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/linux-io-redirections.html" />
  <id>http://localhost:3000/linux-io-redirections</id>
  <published>2016-02-15T00:00:00Z</published>
  <updated>2016-02-15T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>I/O redirections are one of the prettiest things we have in linux (IMO!) Following are commands and their usage.
<br /><br />
<code class="highlighter-rouge">command_output &gt;&gt; file </code>
  Redirects stdout to a file. Creates the file if not present, otherwise appends.</p>

<p><code class="highlighter-rouge"> &gt; filename </code>
  Truncates the file to zero length. If file is not present, creates zero-length file (same effect as <code class="highlighter-rouge">touch</code>).</p>

<p><code class="highlighter-rouge"> 1&gt;filename </code>
  Redirects stdout to the file “filename”.</p>

<p><code class="highlighter-rouge"> 1&gt;&gt;filename </code>
  Redirects and appends stdout to file “filename”.</p>

<p><code class="highlighter-rouge"> 2&gt;filename </code>
  Redirects stderr to file “filename”.</p>

<p><code class="highlighter-rouge"> 2&gt;&gt;filename </code>
  Redirects and appends stderr to file “filename”.</p>

<p><code class="highlighter-rouge"> &amp;&gt;filename </code>
  Redirects both stdout and stderr to file “filename”.</p>

<p><code class="highlighter-rouge"> 2&gt;&amp;1 </code>
  Redirects stderr to stdout. Error messages get sent to same place as standard output.</p>
<hr />

<p>Some quality explanation now ;) Take the example of this command:
<br /><code class="highlighter-rouge"> cmd &gt;&gt; file.log 2&gt;&amp;1 </code>
<br />
This command will redirect all the output of command(cmd) into <code class="highlighter-rouge">file.log</code>.<br />
<code class="highlighter-rouge">2</code> refers to Second file descriptor of the process i.e., stderr<br />
<code class="highlighter-rouge">&gt;</code> refers to redirection<br />
<code class="highlighter-rouge">&amp;1</code> means that the target of redirection would be same as <code class="highlighter-rouge">1</code> i.e, first descriptor i.e, stdout.<br /></p>

<p>Thanks!</p>

<style type="text/css">
code {
    font-weight: bold;
    font-size: 18px;
    background: #ddd;
    padding: 3px;
}   
</style>

 ]]></content>
</entry>


  <entry>
  <title type="text">PostgreSQL - update timestamp when row(s) is updated</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/postgres-update-timestamp-when-row-is-updated.html" />
  <id>http://localhost:3000/postgres-update-timestamp-when-row-is-updated</id>
  <published>2015-11-24T00:00:00Z</published>
  <updated>2015-11-24T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>In PostgreSQL, if you want to set current timestamp as default value, you can simply keep a column’s default expression as <code class="highlighter-rouge">now()</code>. However, by default there is no function defined to update the timestamp when a particular row (or multiple rows) need to be updated.</p>

<p>In such scenario, you may create your custom method and trigger it using <b>PostgreSQL’s Triggers</b>. Following snippet will make it more clear:</p>

<p>Here, we are creating a new method, <code class="highlighter-rouge">method_get_updated_at()</code></p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">OR</span> <span class="k">REPLACE</span> <span class="k">FUNCTION</span> <span class="n">method_get_updated_at</span><span class="p">()</span> <span class="k">RETURNS</span> <span class="k">TRIGGER</span>
<span class="k">LANGUAGE</span> <span class="n">plpgsql</span>
<span class="k">AS</span> <span class="err">$$</span>
    <span class="k">BEGIN</span>
      <span class="k">NEW</span><span class="p">.</span><span class="o">&lt;</span><span class="k">column_name</span><span class="o">&gt;</span> <span class="o">=</span> <span class="n">now</span><span class="p">();</span>
      <span class="k">RETURN</span> <span class="k">NEW</span><span class="p">;</span>
    <span class="k">END</span><span class="p">;</span>
<span class="err">$$</span><span class="p">;</span></code></pre></figure>

<p>Once it is created, use the following snippet to trigger it:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TRIGGER</span> <span class="n">trigger_</span><span class="o">&lt;</span><span class="k">column_name</span><span class="o">&gt;</span>
<span class="k">BEFORE</span> <span class="k">UPDATE</span> <span class="k">ON</span> <span class="o">&lt;</span><span class="k">table_name</span><span class="o">&gt;</span>
<span class="k">FOR</span> <span class="k">EACH</span> <span class="k">ROW</span>
<span class="k">EXECUTE</span> <span class="k">PROCEDURE</span> <span class="n">method_get_updated_at</span><span class="p">();</span></code></pre></figure>

<p>If you want to delete a Trigger, you can use this simple query:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">DROP</span> <span class="k">TRIGGER</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">trigger_</span><span class="o">&lt;</span><span class="k">column_name</span><span class="o">&gt;</span> <span class="k">ON</span> <span class="o">&lt;</span><span class="k">table_name</span><span class="o">&gt;</span></code></pre></figure>

<p><b>Note:</b> Please update the <table_name> and <column_name> accordingly and execute the code for your particular database. Also, note that, some web frameworks (like Rails) manage such columns(created_at, updated_at) automatically.</column_name></table_name></p>

<p>ALso, if you want to view all existing methods, run this query:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span>  <span class="n">p</span><span class="p">.</span><span class="n">proname</span>
<span class="k">FROM</span>    <span class="n">pg_catalog</span><span class="p">.</span><span class="n">pg_namespace</span> <span class="n">n</span>
<span class="k">JOIN</span>    <span class="n">pg_catalog</span><span class="p">.</span><span class="n">pg_proc</span> <span class="n">p</span>
<span class="k">ON</span>      <span class="n">p</span><span class="p">.</span><span class="n">pronamespace</span> <span class="o">=</span> <span class="n">n</span><span class="p">.</span><span class="n">oid</span>
<span class="k">WHERE</span>   <span class="n">n</span><span class="p">.</span><span class="n">nspname</span> <span class="o">=</span> <span class="s1">'public'</span></code></pre></figure>

<p>And, run this query to view all Triggers:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">pg_trigger</span><span class="p">;</span></code></pre></figure>

<p>Thanks!</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">Accessing PostgreSQL server through a SSH Tunnel</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/accessing-postgreSQL-through-ssh-tunnel.html" />
  <id>http://localhost:3000/accessing-postgreSQL-through-ssh-tunnel</id>
  <published>2015-11-20T00:00:00Z</published>
  <updated>2015-11-20T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p><b>Step 1:</b> Check the SSH connectivity with the server, verify username and password.</p>

<p><b>Step 2:</b> Create the tunnel in your local system by executing the following command (It will prompt for password):</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">ssh -fNg -L 5555:localhost:5432 &lt;user&gt;@&lt;server&gt;</code></pre></figure>

<p><b>Step 3:</b> Now, open your PostgreSQL client (eg, <code class="highlighter-rouge">pgAdmin 3</code> or <code class="highlighter-rouge">DBeaver</code> or <code class="highlighter-rouge">Postico</code> for OS X or <code class="highlighter-rouge">Terminal</code>) and fill in the connection details as usual. Check the image below.</p>

<p><img class="img-responsive" src="assets/images/postico-port-forwarding.png" alt="Postico DB connection" /></p>

<p><b>Note:</b> Yes, you’ll have to use <code class="highlighter-rouge">'localhost'</code>.</p>
 ]]></content>
</entry>


  <entry>
  <title type="text">Welcome!</title>
  <link rel="alternate" type="text/html" href="http://localhost:3000/new.html" />
  <id>http://localhost:3000/new</id>
  <published>2011-12-31T00:00:00Z</published>
  <updated>2011-12-31T00:00:00Z</updated>
  <content type="html"><![CDATA[ 
 ]]></content>
</entry>



</feed>
